{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.260215600Z",
     "start_time": "2024-04-23T13:11:59.995777975Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('exported_data/normalized_13.df')\n",
    "# df = pd.read_csv('exported_data/normalized_3.df')\n",
    "\n",
    "\n",
    "PREFIX = \"mteb_ds\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.307860648Z",
     "start_time": "2024-04-23T13:12:00.261927025Z"
    }
   },
   "id": "f201064ec4dbb62c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mteb/stsbenchmark-sts/test/embeddings.npy', 'mteb/sts13-sts/test/embeddings.npy', 'mteb/banking77/test/embeddings.npy', 'snli/validation/embeddings.npy', 'mteb/biosses-sts/test/embeddings.npy', 'snli/test/embeddings.npy', 'mteb/amazon_polarity/test/embeddings.npy', 'mteb/sts12-sts/test/embeddings.npy', 'mteb/stsbenchmark-sts/validation/embeddings.npy', 'mteb/sts15-sts/test/embeddings.npy', 'mteb/sickr-sts/test/embeddings.npy', 'mteb/sts14-sts/test/embeddings.npy']\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for idx, val in df['datasets'].apply(lambda x: sorted(eval(x))).items():\n",
    "    datasets.extend(val)\n",
    "    \n",
    "datasets = list(set(datasets))\n",
    "print(datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.365244532Z",
     "start_time": "2024-04-23T13:12:00.303974433Z"
    }
   },
   "id": "1665ee41ce3562a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                    id                 date  \\\n0           0  dc785306-8cae-4db1-9d55-a8f6434f3803  2024-04-07 16:25:17   \n1           0  5320b1f9-20db-4cbb-b774-c79f9ab1ad90  2024-04-07 14:39:08   \n2           0  f370bf89-e69b-494e-b157-162c76bdd4cf  2024-04-07 19:22:12   \n3           0  a52b3637-e9eb-4fee-810e-054aaa424069  2024-04-07 17:06:28   \n4           0  2b4d8f82-52bf-454e-a6b4-f94e4fd25af1  2024-04-07 14:25:10   \n\n                                             model_1  \\\n0  ../output/sentence-transformers/average_word_e...   \n1                        ../output/llmrails/ember-v1   \n2  ../output/sentence-transformers/msmarco-bert-c...   \n3                          ../output/google/gemma-7b   \n4     ../output/sentence-transformers/sentence-t5-xl   \n\n                                             model_2   d_1   d_2  \\\n0  ../output/sentence-transformers/msmarco-bert-c...   300   768   \n1                        ../output/intfloat/e5-small  1024   384   \n2                       ../output/google/gemma-7b-it   768  3072   \n3  ../output/HuggingFaceM4/tiny-random-LlamaForCa...  3072    16   \n4              ../output/sentence-transformers/LaBSE   768   768   \n\n   dataset_filter                                           datasets  \\\n0             NaN  {'mteb/sts13-sts/test/embeddings.npy', 'mteb/s...   \n1             NaN  {'snli/validation/embeddings.npy', 'mteb/sts14...   \n2             NaN  {'mteb/sts14-sts/test/embeddings.npy', 'mteb/s...   \n3             NaN  {'mteb/stsbenchmark-sts/test/embeddings.npy', ...   \n4             NaN  {'mteb/banking77/test/embeddings.npy', 'mteb/s...   \n\n   I(X_1->X_2)  ...  optimize_mu  cond_modes  marg_modes  use_tanh  init_std  \\\n0   292.661631  ...         True           8           8      True      0.01   \n1   178.770718  ...         True           8           8      True      0.01   \n2  1800.378266  ...         True           8           8      True      0.01   \n3     5.066857  ...         True           8           8      True      0.01   \n4   396.074551  ...         True           8           8      True      0.01   \n\n   ff_residual_connection ff_activation ff_layer_norm  ff_layers  \\\n0                   False          relu          True          2   \n1                   False          relu          True          2   \n2                   False          relu          True          2   \n3                   False          relu          True          2   \n4                   False          relu          True          2   \n\n   ff_dim_hidden  \n0              0  \n1              0  \n2              0  \n3              0  \n4              0  \n\n[5 rows x 35 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>date</th>\n      <th>model_1</th>\n      <th>model_2</th>\n      <th>d_1</th>\n      <th>d_2</th>\n      <th>dataset_filter</th>\n      <th>datasets</th>\n      <th>I(X_1-&gt;X_2)</th>\n      <th>...</th>\n      <th>optimize_mu</th>\n      <th>cond_modes</th>\n      <th>marg_modes</th>\n      <th>use_tanh</th>\n      <th>init_std</th>\n      <th>ff_residual_connection</th>\n      <th>ff_activation</th>\n      <th>ff_layer_norm</th>\n      <th>ff_layers</th>\n      <th>ff_dim_hidden</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>dc785306-8cae-4db1-9d55-a8f6434f3803</td>\n      <td>2024-04-07 16:25:17</td>\n      <td>../output/sentence-transformers/average_word_e...</td>\n      <td>../output/sentence-transformers/msmarco-bert-c...</td>\n      <td>300</td>\n      <td>768</td>\n      <td>NaN</td>\n      <td>{'mteb/sts13-sts/test/embeddings.npy', 'mteb/s...</td>\n      <td>292.661631</td>\n      <td>...</td>\n      <td>True</td>\n      <td>8</td>\n      <td>8</td>\n      <td>True</td>\n      <td>0.01</td>\n      <td>False</td>\n      <td>relu</td>\n      <td>True</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>5320b1f9-20db-4cbb-b774-c79f9ab1ad90</td>\n      <td>2024-04-07 14:39:08</td>\n      <td>../output/llmrails/ember-v1</td>\n      <td>../output/intfloat/e5-small</td>\n      <td>1024</td>\n      <td>384</td>\n      <td>NaN</td>\n      <td>{'snli/validation/embeddings.npy', 'mteb/sts14...</td>\n      <td>178.770718</td>\n      <td>...</td>\n      <td>True</td>\n      <td>8</td>\n      <td>8</td>\n      <td>True</td>\n      <td>0.01</td>\n      <td>False</td>\n      <td>relu</td>\n      <td>True</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>f370bf89-e69b-494e-b157-162c76bdd4cf</td>\n      <td>2024-04-07 19:22:12</td>\n      <td>../output/sentence-transformers/msmarco-bert-c...</td>\n      <td>../output/google/gemma-7b-it</td>\n      <td>768</td>\n      <td>3072</td>\n      <td>NaN</td>\n      <td>{'mteb/sts14-sts/test/embeddings.npy', 'mteb/s...</td>\n      <td>1800.378266</td>\n      <td>...</td>\n      <td>True</td>\n      <td>8</td>\n      <td>8</td>\n      <td>True</td>\n      <td>0.01</td>\n      <td>False</td>\n      <td>relu</td>\n      <td>True</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>a52b3637-e9eb-4fee-810e-054aaa424069</td>\n      <td>2024-04-07 17:06:28</td>\n      <td>../output/google/gemma-7b</td>\n      <td>../output/HuggingFaceM4/tiny-random-LlamaForCa...</td>\n      <td>3072</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>{'mteb/stsbenchmark-sts/test/embeddings.npy', ...</td>\n      <td>5.066857</td>\n      <td>...</td>\n      <td>True</td>\n      <td>8</td>\n      <td>8</td>\n      <td>True</td>\n      <td>0.01</td>\n      <td>False</td>\n      <td>relu</td>\n      <td>True</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2b4d8f82-52bf-454e-a6b4-f94e4fd25af1</td>\n      <td>2024-04-07 14:25:10</td>\n      <td>../output/sentence-transformers/sentence-t5-xl</td>\n      <td>../output/sentence-transformers/LaBSE</td>\n      <td>768</td>\n      <td>768</td>\n      <td>NaN</td>\n      <td>{'mteb/banking77/test/embeddings.npy', 'mteb/s...</td>\n      <td>396.074551</td>\n      <td>...</td>\n      <td>True</td>\n      <td>8</td>\n      <td>8</td>\n      <td>True</td>\n      <td>0.01</td>\n      <td>False</td>\n      <td>relu</td>\n      <td>True</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 35 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.386671749Z",
     "start_time": "2024-04-23T13:12:00.356025262Z"
    }
   },
   "id": "a866f2d5ec29253d",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['I(X_1->X_2)/H(X_2)'] = df['I(X_1->X_2)'] / df['H(X_2)']\n",
    "df['I(X_1->X_2)/d_1'] = df['I(X_1->X_2)'] / df['d_1']\n",
    "df['I(X_1->X_2)/d_2'] = df['I(X_1->X_2)'] / df['d_2']\n",
    "\n",
    "df['model_1'] = df['model_1'].apply(lambda x: \"/\".join(x.split('/')[-2:]))\n",
    "df['model_2'] = df['model_2'].apply(lambda x: \"/\".join(x.split('/')[-2:]))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.429557730Z",
     "start_time": "2024-04-23T13:12:00.373625427Z"
    }
   },
   "id": "5acdcf4072e53d0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = df.groupby(['model_1', 'model_2']).first().reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.473154201Z",
     "start_time": "2024-04-23T13:12:00.380692521Z"
    }
   },
   "id": "10014d147c13ebea",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "df = df[~(df['model_1'] == \"jinaai/jina-embedding-s-en-v1\")]\n",
    "df = df[~(df['model_2'] == \"jinaai/jina-embedding-s-en-v1\")]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.476649527Z",
     "start_time": "2024-04-23T13:12:00.425589748Z"
    }
   },
   "id": "fb19898f47dc657e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44065/3807154875.py:11: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  table = table[[\"model_1\", \"model_2\", METRIC]].pivot(\"model_1\", \"model_2\",METRIC).fillna(0)\n"
     ]
    }
   ],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "METRIC = 'I(X_1->X_2)/d_2'\n",
    "# METRIC = 'I(X_1->X_2)/H(X_2)'\n",
    "# METRIC = 'I(X_1->X_2)'\n",
    "from matplotlib import patheffects\n",
    "\n",
    "cmap =sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "\n",
    "\n",
    "table = df\n",
    "table = table[[\"model_1\", \"model_2\", METRIC]].pivot(\"model_1\", \"model_2\",METRIC).fillna(0)\n",
    "\n",
    "# count number of 0 per row\n",
    "n_0 = table.apply(lambda x: x[x == 0].count(), axis=1)\n",
    "\n",
    "# count number of 0 per column\n",
    "n_0_col = table.apply(lambda x: x[x == 0].count(), axis=0)\n",
    "\n",
    "# remove rows with more than 50% of 0\n",
    "# table = table[n_0 < table.shape[1] // 2]\n",
    "\n",
    "# remove columns with more than 50% of 0\n",
    "# table = table.loc[:, n_0_col < table.shape[0] // 2]\n",
    "\n",
    "# make square\n",
    "# table = table.loc[table.index.intersection(table.columns)]\n",
    "\n",
    "\n",
    "\n",
    "# remove lines with only 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.513973037Z",
     "start_time": "2024-04-23T13:12:00.425886981Z"
    }
   },
   "id": "c804d779e31333d5",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "model_2                                             BAAI/bge-base-en-v1.5  \\\nmodel_1                                                                     \nBAAI/bge-base-en-v1.5                                            1.408755   \nGritLM/GritLM-7B                                                 0.562268   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                       0.073016   \nNousResearch/Llama-2-7b-hf                                       0.523777   \nSalesforce/SFR-Embedding-Mistral                                 0.583908   \nSmartComponents/bge-micro-v2                                     0.511468   \nTaylorAI/gte-tiny                                                0.547937   \nWhereIsAI/UAE-Large-V1                                           0.625907   \navsolatorio/GIST-Embedding-v0                                    1.252645   \ncroissantllm/CroissantCool                                       0.510853   \ncroissantllm/CroissantLLMBase                                    0.511958   \ncroissantllm/base_100k                                           0.511069   \ncroissantllm/base_150k                                           0.506839   \ncroissantllm/base_50k                                            0.504344   \ncroissantllm/base_5k                                             0.501560   \ngoogle/gemma-2b                                                  0.497651   \ngoogle/gemma-2b-it                                               0.522912   \ngoogle/gemma-7b                                                  0.489064   \ngoogle/gemma-7b-it                                               0.534298   \ninfgrad/stella-base-en-v2                                        0.996816   \nintfloat/e5-large-v2                                             0.588100   \nintfloat/e5-small                                                0.531051   \nintfloat/multilingual-e5-small                                   0.524799   \nizhx/udever-bloom-560m                                           0.489196   \njamesgpt1/sf_model_e5                                            0.611252   \njspringer/echo-mistral-7b-instruct-lasttoken                     0.579410   \nllmrails/ember-v1                                                0.622414   \nprinceton-nlp/sup-simcse-bert-base-uncased                       0.526177   \nsentence-transformers/LaBSE                                      0.466823   \nsentence-transformers/all-MiniLM-L6-v2                           0.504232   \nsentence-transformers/all-distilroberta-v1                       0.503751   \nsentence-transformers/all-mpnet-base-v2                          0.501740   \nsentence-transformers/allenai-specter                            0.379962   \nsentence-transformers/average_word_embeddings_g...               0.413889   \nsentence-transformers/average_word_embeddings_k...               0.410048   \nsentence-transformers/gtr-t5-base                                0.521647   \nsentence-transformers/gtr-t5-large                               0.533506   \nsentence-transformers/gtr-t5-xl                                  0.515018   \nsentence-transformers/msmarco-bert-co-condensor                  0.535823   \nsentence-transformers/sentence-t5-large                          0.521697   \nsentence-transformers/sentence-t5-xl                             0.526644   \nthenlper/gte-base                                                1.118852   \nthenlper/gte-large                                               0.618912   \n\nmodel_2                                             GritLM/GritLM-7B  \\\nmodel_1                                                                \nBAAI/bge-base-en-v1.5                                       0.556619   \nGritLM/GritLM-7B                                            0.928033   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                  0.063781   \nNousResearch/Llama-2-7b-hf                                  0.692050   \nSalesforce/SFR-Embedding-Mistral                            0.644836   \nSmartComponents/bge-micro-v2                                0.547267   \nTaylorAI/gte-tiny                                           0.548944   \nWhereIsAI/UAE-Large-V1                                      0.556661   \navsolatorio/GIST-Embedding-v0                               0.559947   \ncroissantllm/CroissantCool                                  0.671090   \ncroissantllm/CroissantLLMBase                               0.663391   \ncroissantllm/base_100k                                      0.671474   \ncroissantllm/base_150k                                      0.648802   \ncroissantllm/base_50k                                       0.677135   \ncroissantllm/base_5k                                        0.630032   \ngoogle/gemma-2b                                             0.685574   \ngoogle/gemma-2b-it                                          0.698448   \ngoogle/gemma-7b                                             0.650991   \ngoogle/gemma-7b-it                                          0.705515   \ninfgrad/stella-base-en-v2                                   0.573110   \nintfloat/e5-large-v2                                        0.600249   \nintfloat/e5-small                                           0.567639   \nintfloat/multilingual-e5-small                              0.555619   \nizhx/udever-bloom-560m                                      0.618040   \njamesgpt1/sf_model_e5                                       0.567582   \njspringer/echo-mistral-7b-instruct-lasttoken                0.798141   \nllmrails/ember-v1                                           0.577960   \nprinceton-nlp/sup-simcse-bert-base-uncased                  0.541301   \nsentence-transformers/LaBSE                                 0.545317   \nsentence-transformers/all-MiniLM-L6-v2                      0.525694   \nsentence-transformers/all-distilroberta-v1                  0.558081   \nsentence-transformers/all-mpnet-base-v2                     0.534215   \nsentence-transformers/allenai-specter                       0.434403   \nsentence-transformers/average_word_embeddings_g...          0.462658   \nsentence-transformers/average_word_embeddings_k...          0.469406   \nsentence-transformers/gtr-t5-base                           0.544015   \nsentence-transformers/gtr-t5-large                          0.593557   \nsentence-transformers/gtr-t5-xl                             0.577901   \nsentence-transformers/msmarco-bert-co-condensor             0.540151   \nsentence-transformers/sentence-t5-large                     0.565155   \nsentence-transformers/sentence-t5-xl                        0.565993   \nthenlper/gte-base                                           0.565083   \nthenlper/gte-large                                          0.594474   \n\nmodel_2                                             HuggingFaceM4/tiny-random-LlamaForCausalLM  \\\nmodel_1                                                                                          \nBAAI/bge-base-en-v1.5                                                                 0.216323   \nGritLM/GritLM-7B                                                                      0.370918   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                            1.128528   \nNousResearch/Llama-2-7b-hf                                                            0.364495   \nSalesforce/SFR-Embedding-Mistral                                                      0.298796   \nSmartComponents/bge-micro-v2                                                          0.206421   \nTaylorAI/gte-tiny                                                                     0.198077   \nWhereIsAI/UAE-Large-V1                                                                0.199992   \navsolatorio/GIST-Embedding-v0                                                         0.207721   \ncroissantllm/CroissantCool                                                            0.314416   \ncroissantllm/CroissantLLMBase                                                         0.316978   \ncroissantllm/base_100k                                                                0.315959   \ncroissantllm/base_150k                                                                0.318091   \ncroissantllm/base_50k                                                                 0.319642   \ncroissantllm/base_5k                                                                  0.328694   \ngoogle/gemma-2b                                                                       0.305570   \ngoogle/gemma-2b-it                                                                    0.313725   \ngoogle/gemma-7b                                                                       0.316679   \ngoogle/gemma-7b-it                                                                    0.338456   \ninfgrad/stella-base-en-v2                                                             0.223171   \nintfloat/e5-large-v2                                                                  0.217827   \nintfloat/e5-small                                                                     0.192635   \nintfloat/multilingual-e5-small                                                        0.202738   \nizhx/udever-bloom-560m                                                                0.292172   \njamesgpt1/sf_model_e5                                                                 0.211733   \njspringer/echo-mistral-7b-instruct-lasttoken                                          0.384352   \nllmrails/ember-v1                                                                     0.216342   \nprinceton-nlp/sup-simcse-bert-base-uncased                                            0.250812   \nsentence-transformers/LaBSE                                                           0.225060   \nsentence-transformers/all-MiniLM-L6-v2                                                0.184804   \nsentence-transformers/all-distilroberta-v1                                            0.199878   \nsentence-transformers/all-mpnet-base-v2                                               0.181181   \nsentence-transformers/allenai-specter                                                 0.201173   \nsentence-transformers/average_word_embeddings_g...                                    0.138756   \nsentence-transformers/average_word_embeddings_k...                                    0.150245   \nsentence-transformers/gtr-t5-base                                                     0.204846   \nsentence-transformers/gtr-t5-large                                                    0.208719   \nsentence-transformers/gtr-t5-xl                                                       0.204779   \nsentence-transformers/msmarco-bert-co-condensor                                       0.225705   \nsentence-transformers/sentence-t5-large                                               0.197310   \nsentence-transformers/sentence-t5-xl                                                  0.194660   \nthenlper/gte-base                                                                     0.206459   \nthenlper/gte-large                                                                    0.203882   \n\nmodel_2                                             NousResearch/Llama-2-7b-hf  \\\nmodel_1                                                                          \nBAAI/bge-base-en-v1.5                                                 0.658130   \nGritLM/GritLM-7B                                                      0.866750   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                            0.089295   \nNousResearch/Llama-2-7b-hf                                            1.022687   \nSalesforce/SFR-Embedding-Mistral                                      0.729444   \nSmartComponents/bge-micro-v2                                          0.635582   \nTaylorAI/gte-tiny                                                     0.640014   \nWhereIsAI/UAE-Large-V1                                                0.634317   \navsolatorio/GIST-Embedding-v0                                         0.655228   \ncroissantllm/CroissantCool                                            0.798290   \ncroissantllm/CroissantLLMBase                                         0.797183   \ncroissantllm/base_100k                                                0.797092   \ncroissantllm/base_150k                                                0.795367   \ncroissantllm/base_50k                                                 0.796211   \ncroissantllm/base_5k                                                  0.738761   \ngoogle/gemma-2b                                                       0.819559   \ngoogle/gemma-2b-it                                                    0.797754   \ngoogle/gemma-7b                                                       0.771033   \ngoogle/gemma-7b-it                                                    0.816301   \ninfgrad/stella-base-en-v2                                             0.651793   \nintfloat/e5-large-v2                                                  0.687747   \nintfloat/e5-small                                                     0.648732   \nintfloat/multilingual-e5-small                                        0.642422   \nizhx/udever-bloom-560m                                                0.710372   \njamesgpt1/sf_model_e5                                                 0.647158   \njspringer/echo-mistral-7b-instruct-lasttoken                          0.862959   \nllmrails/ember-v1                                                     0.662653   \nprinceton-nlp/sup-simcse-bert-base-uncased                            0.630290   \nsentence-transformers/LaBSE                                           0.651559   \nsentence-transformers/all-MiniLM-L6-v2                                0.624404   \nsentence-transformers/all-distilroberta-v1                            0.630000   \nsentence-transformers/all-mpnet-base-v2                               0.611461   \nsentence-transformers/allenai-specter                                 0.490597   \nsentence-transformers/average_word_embeddings_g...                    0.538345   \nsentence-transformers/average_word_embeddings_k...                    0.549655   \nsentence-transformers/gtr-t5-base                                     0.659980   \nsentence-transformers/gtr-t5-large                                    0.667555   \nsentence-transformers/gtr-t5-xl                                       0.680401   \nsentence-transformers/msmarco-bert-co-condensor                       0.651980   \nsentence-transformers/sentence-t5-large                               0.655143   \nsentence-transformers/sentence-t5-xl                                  0.629109   \nthenlper/gte-base                                                     0.666170   \nthenlper/gte-large                                                    0.662110   \n\nmodel_2                                             Salesforce/SFR-Embedding-Mistral  \\\nmodel_1                                                                                \nBAAI/bge-base-en-v1.5                                                       0.627063   \nGritLM/GritLM-7B                                                            0.665229   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                  0.033481   \nNousResearch/Llama-2-7b-hf                                                  0.548256   \nSalesforce/SFR-Embedding-Mistral                                            1.141443   \nSmartComponents/bge-micro-v2                                                0.620145   \nTaylorAI/gte-tiny                                                           0.607331   \nWhereIsAI/UAE-Large-V1                                                      0.646123   \navsolatorio/GIST-Embedding-v0                                               0.630045   \ncroissantllm/CroissantCool                                                  0.543304   \ncroissantllm/CroissantLLMBase                                               0.525400   \ncroissantllm/base_100k                                                      0.525701   \ncroissantllm/base_150k                                                      0.540232   \ncroissantllm/base_50k                                                       0.540682   \ncroissantllm/base_5k                                                        0.506305   \ngoogle/gemma-2b                                                             0.544654   \ngoogle/gemma-2b-it                                                          0.597168   \ngoogle/gemma-7b                                                             0.443719   \ngoogle/gemma-7b-it                                                          0.602809   \ninfgrad/stella-base-en-v2                                                   0.639125   \nintfloat/e5-large-v2                                                        0.693649   \nintfloat/e5-small                                                           0.605417   \nintfloat/multilingual-e5-small                                              0.647128   \nizhx/udever-bloom-560m                                                      0.550806   \njamesgpt1/sf_model_e5                                                       0.632201   \njspringer/echo-mistral-7b-instruct-lasttoken                                0.718967   \nllmrails/ember-v1                                                           0.628820   \nprinceton-nlp/sup-simcse-bert-base-uncased                                  0.565965   \nsentence-transformers/LaBSE                                                 0.542894   \nsentence-transformers/all-MiniLM-L6-v2                                      0.604181   \nsentence-transformers/all-distilroberta-v1                                  0.599850   \nsentence-transformers/all-mpnet-base-v2                                     0.617648   \nsentence-transformers/allenai-specter                                       0.413534   \nsentence-transformers/average_word_embeddings_g...                          0.467645   \nsentence-transformers/average_word_embeddings_k...                          0.500383   \nsentence-transformers/gtr-t5-base                                           0.657463   \nsentence-transformers/gtr-t5-large                                          0.672884   \nsentence-transformers/gtr-t5-xl                                             0.685843   \nsentence-transformers/msmarco-bert-co-condensor                             0.595589   \nsentence-transformers/sentence-t5-large                                     0.649616   \nsentence-transformers/sentence-t5-xl                                        0.680702   \nthenlper/gte-base                                                           0.644759   \nthenlper/gte-large                                                          0.657575   \n\nmodel_2                                             SmartComponents/bge-micro-v2  \\\nmodel_1                                                                            \nBAAI/bge-base-en-v1.5                                                   0.505610   \nGritLM/GritLM-7B                                                        0.512743   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                              0.071567   \nNousResearch/Llama-2-7b-hf                                              0.475209   \nSalesforce/SFR-Embedding-Mistral                                        0.541588   \nSmartComponents/bge-micro-v2                                            1.410436   \nTaylorAI/gte-tiny                                                       0.806125   \nWhereIsAI/UAE-Large-V1                                                  0.501665   \navsolatorio/GIST-Embedding-v0                                           0.524688   \ncroissantllm/CroissantCool                                              0.455690   \ncroissantllm/CroissantLLMBase                                           0.457884   \ncroissantllm/base_100k                                                  0.457018   \ncroissantllm/base_150k                                                  0.463220   \ncroissantllm/base_50k                                                   0.461757   \ncroissantllm/base_5k                                                    0.464391   \ngoogle/gemma-2b                                                         0.448804   \ngoogle/gemma-2b-it                                                      0.474761   \ngoogle/gemma-7b                                                         0.439864   \ngoogle/gemma-7b-it                                                      0.491458   \ninfgrad/stella-base-en-v2                                               0.555202   \nintfloat/e5-large-v2                                                    0.503315   \nintfloat/e5-small                                                       0.510709   \nintfloat/multilingual-e5-small                                          0.498778   \nizhx/udever-bloom-560m                                                  0.466338   \njamesgpt1/sf_model_e5                                                   0.508962   \njspringer/echo-mistral-7b-instruct-lasttoken                            0.534985   \nllmrails/ember-v1                                                       0.505790   \nprinceton-nlp/sup-simcse-bert-base-uncased                              0.474583   \nsentence-transformers/LaBSE                                             0.456566   \nsentence-transformers/all-MiniLM-L6-v2                                  0.507903   \nsentence-transformers/all-distilroberta-v1                              0.464963   \nsentence-transformers/all-mpnet-base-v2                                 0.436941   \nsentence-transformers/allenai-specter                                   0.364405   \nsentence-transformers/average_word_embeddings_g...                      0.400742   \nsentence-transformers/average_word_embeddings_k...                      0.406297   \nsentence-transformers/gtr-t5-base                                       0.482473   \nsentence-transformers/gtr-t5-large                                      0.478711   \nsentence-transformers/gtr-t5-xl                                         0.472509   \nsentence-transformers/msmarco-bert-co-condensor                         0.487896   \nsentence-transformers/sentence-t5-large                                 0.460349   \nsentence-transformers/sentence-t5-xl                                    0.443204   \nthenlper/gte-base                                                       0.529239   \nthenlper/gte-large                                                      0.507029   \n\nmodel_2                                             TaylorAI/gte-tiny  \\\nmodel_1                                                                 \nBAAI/bge-base-en-v1.5                                        0.518470   \nGritLM/GritLM-7B                                             0.502282   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                   0.069533   \nNousResearch/Llama-2-7b-hf                                   0.462156   \nSalesforce/SFR-Embedding-Mistral                             0.532534   \nSmartComponents/bge-micro-v2                                 0.798591   \nTaylorAI/gte-tiny                                            1.424008   \nWhereIsAI/UAE-Large-V1                                       0.507954   \navsolatorio/GIST-Embedding-v0                                0.532512   \ncroissantllm/CroissantCool                                   0.437453   \ncroissantllm/CroissantLLMBase                                0.443562   \ncroissantllm/base_100k                                       0.445600   \ncroissantllm/base_150k                                       0.440017   \ncroissantllm/base_50k                                        0.446798   \ncroissantllm/base_5k                                         0.446950   \ngoogle/gemma-2b                                              0.426255   \ngoogle/gemma-2b-it                                           0.450217   \ngoogle/gemma-7b                                              0.429306   \ngoogle/gemma-7b-it                                           0.463196   \ninfgrad/stella-base-en-v2                                    0.554735   \nintfloat/e5-large-v2                                         0.513624   \nintfloat/e5-small                                            0.503236   \nintfloat/multilingual-e5-small                               0.489159   \nizhx/udever-bloom-560m                                       0.452572   \njamesgpt1/sf_model_e5                                        0.502087   \njspringer/echo-mistral-7b-instruct-lasttoken                 0.521874   \nllmrails/ember-v1                                            0.504016   \nprinceton-nlp/sup-simcse-bert-base-uncased                   0.448323   \nsentence-transformers/LaBSE                                  0.433137   \nsentence-transformers/all-MiniLM-L6-v2                       0.489828   \nsentence-transformers/all-distilroberta-v1                   0.454726   \nsentence-transformers/all-mpnet-base-v2                      0.431897   \nsentence-transformers/allenai-specter                        0.344119   \nsentence-transformers/average_word_embeddings_g...           0.379588   \nsentence-transformers/average_word_embeddings_k...           0.381755   \nsentence-transformers/gtr-t5-base                            0.454860   \nsentence-transformers/gtr-t5-large                           0.473222   \nsentence-transformers/gtr-t5-xl                              0.462295   \nsentence-transformers/msmarco-bert-co-condensor              0.468041   \nsentence-transformers/sentence-t5-large                      0.447484   \nsentence-transformers/sentence-t5-xl                         0.439632   \nthenlper/gte-base                                            0.541013   \nthenlper/gte-large                                           0.532981   \n\nmodel_2                                             WhereIsAI/UAE-Large-V1  \\\nmodel_1                                                                      \nBAAI/bge-base-en-v1.5                                             0.672778   \nGritLM/GritLM-7B                                                  0.613329   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                        0.067744   \nNousResearch/Llama-2-7b-hf                                        0.576038   \nSalesforce/SFR-Embedding-Mistral                                  0.648821   \nSmartComponents/bge-micro-v2                                      0.553717   \nTaylorAI/gte-tiny                                                 0.563739   \nWhereIsAI/UAE-Large-V1                                            1.413947   \navsolatorio/GIST-Embedding-v0                                     0.680945   \ncroissantllm/CroissantCool                                        0.560995   \ncroissantllm/CroissantLLMBase                                     0.552638   \ncroissantllm/base_100k                                            0.559053   \ncroissantllm/base_150k                                            0.552501   \ncroissantllm/base_50k                                             0.554349   \ncroissantllm/base_5k                                              0.529942   \ngoogle/gemma-2b                                                   0.539649   \ngoogle/gemma-2b-it                                                0.576367   \ngoogle/gemma-7b                                                   0.526452   \ngoogle/gemma-7b-it                                                0.595353   \ninfgrad/stella-base-en-v2                                         0.729123   \nintfloat/e5-large-v2                                              0.630756   \nintfloat/e5-small                                                 0.558432   \nintfloat/multilingual-e5-small                                    0.561169   \nizhx/udever-bloom-560m                                            0.523653   \njamesgpt1/sf_model_e5                                             1.243419   \njspringer/echo-mistral-7b-instruct-lasttoken                      0.634924   \nllmrails/ember-v1                                                 1.226934   \nprinceton-nlp/sup-simcse-bert-base-uncased                        0.538649   \nsentence-transformers/LaBSE                                       0.505085   \nsentence-transformers/all-MiniLM-L6-v2                            0.530816   \nsentence-transformers/all-distilroberta-v1                        0.560070   \nsentence-transformers/all-mpnet-base-v2                           0.547799   \nsentence-transformers/allenai-specter                             0.412861   \nsentence-transformers/average_word_embeddings_g...                0.448311   \nsentence-transformers/average_word_embeddings_k...                0.452204   \nsentence-transformers/gtr-t5-base                                 0.574804   \nsentence-transformers/gtr-t5-large                                0.587680   \nsentence-transformers/gtr-t5-xl                                   0.588710   \nsentence-transformers/msmarco-bert-co-condensor                   0.535067   \nsentence-transformers/sentence-t5-large                           0.602207   \nsentence-transformers/sentence-t5-xl                              0.556886   \nthenlper/gte-base                                                 0.686315   \nthenlper/gte-large                                                1.174485   \n\nmodel_2                                             avsolatorio/GIST-Embedding-v0  \\\nmodel_1                                                                             \nBAAI/bge-base-en-v1.5                                                    1.279503   \nGritLM/GritLM-7B                                                         0.591918   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                               0.069579   \nNousResearch/Llama-2-7b-hf                                               0.546495   \nSalesforce/SFR-Embedding-Mistral                                         0.612515   \nSmartComponents/bge-micro-v2                                             0.549635   \nTaylorAI/gte-tiny                                                        0.574487   \nWhereIsAI/UAE-Large-V1                                                   0.667389   \navsolatorio/GIST-Embedding-v0                                            1.427954   \ncroissantllm/CroissantCool                                               0.528483   \ncroissantllm/CroissantLLMBase                                            0.529074   \ncroissantllm/base_100k                                                   0.514522   \ncroissantllm/base_150k                                                   0.526479   \ncroissantllm/base_50k                                                    0.534027   \ncroissantllm/base_5k                                                     0.510975   \ngoogle/gemma-2b                                                          0.516216   \ngoogle/gemma-2b-it                                                       0.552545   \ngoogle/gemma-7b                                                          0.511481   \ngoogle/gemma-7b-it                                                       0.563139   \ninfgrad/stella-base-en-v2                                                0.998513   \nintfloat/e5-large-v2                                                     0.605085   \nintfloat/e5-small                                                        0.548294   \nintfloat/multilingual-e5-small                                           0.526664   \nizhx/udever-bloom-560m                                                   0.507423   \njamesgpt1/sf_model_e5                                                    0.647557   \njspringer/echo-mistral-7b-instruct-lasttoken                             0.598169   \nllmrails/ember-v1                                                        0.658961   \nprinceton-nlp/sup-simcse-bert-base-uncased                               0.547264   \nsentence-transformers/LaBSE                                              0.479587   \nsentence-transformers/all-MiniLM-L6-v2                                   0.525732   \nsentence-transformers/all-distilroberta-v1                               0.546914   \nsentence-transformers/all-mpnet-base-v2                                  0.531392   \nsentence-transformers/allenai-specter                                    0.393944   \nsentence-transformers/average_word_embeddings_g...                       0.427350   \nsentence-transformers/average_word_embeddings_k...                       0.435028   \nsentence-transformers/gtr-t5-base                                        0.528916   \nsentence-transformers/gtr-t5-large                                       0.541225   \nsentence-transformers/gtr-t5-xl                                          0.546713   \nsentence-transformers/msmarco-bert-co-condensor                          0.525086   \nsentence-transformers/sentence-t5-large                                  0.542266   \nsentence-transformers/sentence-t5-xl                                     0.538152   \nthenlper/gte-base                                                        1.198267   \nthenlper/gte-large                                                       0.674558   \n\nmodel_2                                             croissantllm/CroissantCool  \\\nmodel_1                                                                          \nBAAI/bge-base-en-v1.5                                                 0.580892   \nGritLM/GritLM-7B                                                      0.780834   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                            0.089979   \nNousResearch/Llama-2-7b-hf                                            0.759894   \nSalesforce/SFR-Embedding-Mistral                                      0.638281   \nSmartComponents/bge-micro-v2                                          0.571141   \nTaylorAI/gte-tiny                                                     0.578049   \nWhereIsAI/UAE-Large-V1                                                0.588979   \navsolatorio/GIST-Embedding-v0                                         0.588988   \ncroissantllm/CroissantCool                                            1.254203   \ncroissantllm/CroissantLLMBase                                         0.975226   \ncroissantllm/base_100k                                                0.917230   \ncroissantllm/base_150k                                                0.957526   \ncroissantllm/base_50k                                                 0.859379   \ncroissantllm/base_5k                                                  0.756661   \ngoogle/gemma-2b                                                       0.728332   \ngoogle/gemma-2b-it                                                    0.753346   \ngoogle/gemma-7b                                                       0.701325   \ngoogle/gemma-7b-it                                                    0.747503   \ninfgrad/stella-base-en-v2                                             0.602653   \nintfloat/e5-large-v2                                                  0.607066   \nintfloat/e5-small                                                     0.577086   \nintfloat/multilingual-e5-small                                        0.577948   \nizhx/udever-bloom-560m                                                0.680482   \njamesgpt1/sf_model_e5                                                 0.590236   \njspringer/echo-mistral-7b-instruct-lasttoken                          0.785881   \nllmrails/ember-v1                                                     0.584525   \nprinceton-nlp/sup-simcse-bert-base-uncased                            0.586227   \nsentence-transformers/LaBSE                                           0.595097   \nsentence-transformers/all-MiniLM-L6-v2                                0.539742   \nsentence-transformers/all-distilroberta-v1                            0.566146   \nsentence-transformers/all-mpnet-base-v2                               0.549363   \nsentence-transformers/allenai-specter                                 0.461949   \nsentence-transformers/average_word_embeddings_g...                    0.496353   \nsentence-transformers/average_word_embeddings_k...                    0.496225   \nsentence-transformers/gtr-t5-base                                     0.597972   \nsentence-transformers/gtr-t5-large                                    0.597531   \nsentence-transformers/gtr-t5-xl                                       0.592024   \nsentence-transformers/msmarco-bert-co-condensor                       0.590187   \nsentence-transformers/sentence-t5-large                               0.584934   \nsentence-transformers/sentence-t5-xl                                  0.578010   \nthenlper/gte-base                                                     0.594267   \nthenlper/gte-large                                                    0.592834   \n\nmodel_2                                             ...  \\\nmodel_1                                             ...   \nBAAI/bge-base-en-v1.5                               ...   \nGritLM/GritLM-7B                                    ...   \nHuggingFaceM4/tiny-random-LlamaForCausalLM          ...   \nNousResearch/Llama-2-7b-hf                          ...   \nSalesforce/SFR-Embedding-Mistral                    ...   \nSmartComponents/bge-micro-v2                        ...   \nTaylorAI/gte-tiny                                   ...   \nWhereIsAI/UAE-Large-V1                              ...   \navsolatorio/GIST-Embedding-v0                       ...   \ncroissantllm/CroissantCool                          ...   \ncroissantllm/CroissantLLMBase                       ...   \ncroissantllm/base_100k                              ...   \ncroissantllm/base_150k                              ...   \ncroissantllm/base_50k                               ...   \ncroissantllm/base_5k                                ...   \ngoogle/gemma-2b                                     ...   \ngoogle/gemma-2b-it                                  ...   \ngoogle/gemma-7b                                     ...   \ngoogle/gemma-7b-it                                  ...   \ninfgrad/stella-base-en-v2                           ...   \nintfloat/e5-large-v2                                ...   \nintfloat/e5-small                                   ...   \nintfloat/multilingual-e5-small                      ...   \nizhx/udever-bloom-560m                              ...   \njamesgpt1/sf_model_e5                               ...   \njspringer/echo-mistral-7b-instruct-lasttoken        ...   \nllmrails/ember-v1                                   ...   \nprinceton-nlp/sup-simcse-bert-base-uncased          ...   \nsentence-transformers/LaBSE                         ...   \nsentence-transformers/all-MiniLM-L6-v2              ...   \nsentence-transformers/all-distilroberta-v1          ...   \nsentence-transformers/all-mpnet-base-v2             ...   \nsentence-transformers/allenai-specter               ...   \nsentence-transformers/average_word_embeddings_g...  ...   \nsentence-transformers/average_word_embeddings_k...  ...   \nsentence-transformers/gtr-t5-base                   ...   \nsentence-transformers/gtr-t5-large                  ...   \nsentence-transformers/gtr-t5-xl                     ...   \nsentence-transformers/msmarco-bert-co-condensor     ...   \nsentence-transformers/sentence-t5-large             ...   \nsentence-transformers/sentence-t5-xl                ...   \nthenlper/gte-base                                   ...   \nthenlper/gte-large                                  ...   \n\nmodel_2                                             sentence-transformers/average_word_embeddings_glove.6B.300d  \\\nmodel_1                                                                                                           \nBAAI/bge-base-en-v1.5                                                                        0.498936             \nGritLM/GritLM-7B                                                                             0.556971             \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                                   0.115742             \nNousResearch/Llama-2-7b-hf                                                                   0.533015             \nSalesforce/SFR-Embedding-Mistral                                                             0.540469             \nSmartComponents/bge-micro-v2                                                                 0.520413             \nTaylorAI/gte-tiny                                                                            0.503811             \nWhereIsAI/UAE-Large-V1                                                                       0.488767             \navsolatorio/GIST-Embedding-v0                                                                0.494188             \ncroissantllm/CroissantCool                                                                   0.505237             \ncroissantllm/CroissantLLMBase                                                                0.505339             \ncroissantllm/base_100k                                                                       0.505628             \ncroissantllm/base_150k                                                                       0.513175             \ncroissantllm/base_50k                                                                        0.510845             \ncroissantllm/base_5k                                                                         0.523563             \ngoogle/gemma-2b                                                                              0.495073             \ngoogle/gemma-2b-it                                                                           0.525578             \ngoogle/gemma-7b                                                                              0.493149             \ngoogle/gemma-7b-it                                                                           0.526362             \ninfgrad/stella-base-en-v2                                                                    0.518842             \nintfloat/e5-large-v2                                                                         0.506488             \nintfloat/e5-small                                                                            0.487990             \nintfloat/multilingual-e5-small                                                               0.484153             \nizhx/udever-bloom-560m                                                                       0.514319             \njamesgpt1/sf_model_e5                                                                        0.479518             \njspringer/echo-mistral-7b-instruct-lasttoken                                                 0.580162             \nllmrails/ember-v1                                                                            0.488522             \nprinceton-nlp/sup-simcse-bert-base-uncased                                                   0.518778             \nsentence-transformers/LaBSE                                                                  0.496056             \nsentence-transformers/all-MiniLM-L6-v2                                                       0.466498             \nsentence-transformers/all-distilroberta-v1                                                   0.464665             \nsentence-transformers/all-mpnet-base-v2                                                      0.441049             \nsentence-transformers/allenai-specter                                                        0.404639             \nsentence-transformers/average_word_embeddings_g...                                           1.223311             \nsentence-transformers/average_word_embeddings_k...                                           0.749655             \nsentence-transformers/gtr-t5-base                                                            0.482651             \nsentence-transformers/gtr-t5-large                                                           0.495393             \nsentence-transformers/gtr-t5-xl                                                              0.489431             \nsentence-transformers/msmarco-bert-co-condensor                                              0.494336             \nsentence-transformers/sentence-t5-large                                                      0.467775             \nsentence-transformers/sentence-t5-xl                                                         0.456685             \nthenlper/gte-base                                                                            0.495669             \nthenlper/gte-large                                                                           0.491538             \n\nmodel_2                                             sentence-transformers/average_word_embeddings_komninos  \\\nmodel_1                                                                                                      \nBAAI/bge-base-en-v1.5                                                                        0.445808        \nGritLM/GritLM-7B                                                                             0.537035        \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                                   0.105286        \nNousResearch/Llama-2-7b-hf                                                                   0.499705        \nSalesforce/SFR-Embedding-Mistral                                                             0.499941        \nSmartComponents/bge-micro-v2                                                                 0.468280        \nTaylorAI/gte-tiny                                                                            0.460555        \nWhereIsAI/UAE-Large-V1                                                                       0.440581        \navsolatorio/GIST-Embedding-v0                                                                0.449655        \ncroissantllm/CroissantCool                                                                   0.473526        \ncroissantllm/CroissantLLMBase                                                                0.474057        \ncroissantllm/base_100k                                                                       0.478196        \ncroissantllm/base_150k                                                                       0.470818        \ncroissantllm/base_50k                                                                        0.486585        \ncroissantllm/base_5k                                                                         0.489645        \ngoogle/gemma-2b                                                                              0.468358        \ngoogle/gemma-2b-it                                                                           0.492195        \ngoogle/gemma-7b                                                                              0.454896        \ngoogle/gemma-7b-it                                                                           0.504533        \ninfgrad/stella-base-en-v2                                                                    0.470766        \nintfloat/e5-large-v2                                                                         0.460488        \nintfloat/e5-small                                                                            0.442540        \nintfloat/multilingual-e5-small                                                               0.452542        \nizhx/udever-bloom-560m                                                                       0.482460        \njamesgpt1/sf_model_e5                                                                        0.444600        \njspringer/echo-mistral-7b-instruct-lasttoken                                                 0.533257        \nllmrails/ember-v1                                                                            0.449348        \nprinceton-nlp/sup-simcse-bert-base-uncased                                                   0.481821        \nsentence-transformers/LaBSE                                                                  0.456030        \nsentence-transformers/all-MiniLM-L6-v2                                                       0.420341        \nsentence-transformers/all-distilroberta-v1                                                   0.420933        \nsentence-transformers/all-mpnet-base-v2                                                      0.396499        \nsentence-transformers/allenai-specter                                                        0.370386        \nsentence-transformers/average_word_embeddings_g...                                           0.653541        \nsentence-transformers/average_word_embeddings_k...                                           1.234867        \nsentence-transformers/gtr-t5-base                                                            0.444268        \nsentence-transformers/gtr-t5-large                                                           0.452145        \nsentence-transformers/gtr-t5-xl                                                              0.445010        \nsentence-transformers/msmarco-bert-co-condensor                                              0.460483        \nsentence-transformers/sentence-t5-large                                                      0.416189        \nsentence-transformers/sentence-t5-xl                                                         0.409267        \nthenlper/gte-base                                                                            0.451392        \nthenlper/gte-large                                                                           0.443071        \n\nmodel_2                                             sentence-transformers/gtr-t5-base  \\\nmodel_1                                                                                 \nBAAI/bge-base-en-v1.5                                                        0.476813   \nGritLM/GritLM-7B                                                             0.506658   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                   0.056679   \nNousResearch/Llama-2-7b-hf                                                   0.464317   \nSalesforce/SFR-Embedding-Mistral                                             0.548169   \nSmartComponents/bge-micro-v2                                                 0.443943   \nTaylorAI/gte-tiny                                                            0.442818   \nWhereIsAI/UAE-Large-V1                                                       0.465090   \navsolatorio/GIST-Embedding-v0                                                0.459249   \ncroissantllm/CroissantCool                                                   0.454472   \ncroissantllm/CroissantLLMBase                                                0.446682   \ncroissantllm/base_100k                                                       0.444599   \ncroissantllm/base_150k                                                       0.445761   \ncroissantllm/base_50k                                                        0.456624   \ncroissantllm/base_5k                                                         0.449824   \ngoogle/gemma-2b                                                              0.442275   \ngoogle/gemma-2b-it                                                           0.459755   \ngoogle/gemma-7b                                                              0.431446   \ngoogle/gemma-7b-it                                                           0.488652   \ninfgrad/stella-base-en-v2                                                    0.492605   \nintfloat/e5-large-v2                                                         0.499514   \nintfloat/e5-small                                                            0.461136   \nintfloat/multilingual-e5-small                                               0.470881   \nizhx/udever-bloom-560m                                                       0.436873   \njamesgpt1/sf_model_e5                                                        0.458527   \njspringer/echo-mistral-7b-instruct-lasttoken                                 0.535313   \nllmrails/ember-v1                                                            0.463220   \nprinceton-nlp/sup-simcse-bert-base-uncased                                   0.418486   \nsentence-transformers/LaBSE                                                  0.403749   \nsentence-transformers/all-MiniLM-L6-v2                                       0.418989   \nsentence-transformers/all-distilroberta-v1                                   0.464351   \nsentence-transformers/all-mpnet-base-v2                                      0.433293   \nsentence-transformers/allenai-specter                                        0.330406   \nsentence-transformers/average_word_embeddings_g...                           0.352274   \nsentence-transformers/average_word_embeddings_k...                           0.358267   \nsentence-transformers/gtr-t5-base                                            1.423988   \nsentence-transformers/gtr-t5-large                                           0.638125   \nsentence-transformers/gtr-t5-xl                                              0.584445   \nsentence-transformers/msmarco-bert-co-condensor                              0.462056   \nsentence-transformers/sentence-t5-large                                      0.511749   \nsentence-transformers/sentence-t5-xl                                         0.484659   \nthenlper/gte-base                                                            0.465789   \nthenlper/gte-large                                                           0.482341   \n\nmodel_2                                             sentence-transformers/gtr-t5-large  \\\nmodel_1                                                                                  \nBAAI/bge-base-en-v1.5                                                         0.465046   \nGritLM/GritLM-7B                                                              0.514760   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                    0.058115   \nNousResearch/Llama-2-7b-hf                                                    0.459331   \nSalesforce/SFR-Embedding-Mistral                                              0.555192   \nSmartComponents/bge-micro-v2                                                  0.413541   \nTaylorAI/gte-tiny                                                             0.413936   \nWhereIsAI/UAE-Large-V1                                                        0.460625   \navsolatorio/GIST-Embedding-v0                                                 0.451516   \ncroissantllm/CroissantCool                                                    0.438180   \ncroissantllm/CroissantLLMBase                                                 0.441439   \ncroissantllm/base_100k                                                        0.450415   \ncroissantllm/base_150k                                                        0.451087   \ncroissantllm/base_50k                                                         0.444806   \ncroissantllm/base_5k                                                          0.424147   \ngoogle/gemma-2b                                                               0.436050   \ngoogle/gemma-2b-it                                                            0.461056   \ngoogle/gemma-7b                                                               0.434091   \ngoogle/gemma-7b-it                                                            0.483912   \ninfgrad/stella-base-en-v2                                                     0.487023   \nintfloat/e5-large-v2                                                          0.504611   \nintfloat/e5-small                                                             0.446859   \nintfloat/multilingual-e5-small                                                0.446955   \nizhx/udever-bloom-560m                                                        0.419580   \njamesgpt1/sf_model_e5                                                         0.476782   \njspringer/echo-mistral-7b-instruct-lasttoken                                  0.513519   \nllmrails/ember-v1                                                             0.462539   \nprinceton-nlp/sup-simcse-bert-base-uncased                                    0.408098   \nsentence-transformers/LaBSE                                                   0.400974   \nsentence-transformers/all-MiniLM-L6-v2                                        0.416894   \nsentence-transformers/all-distilroberta-v1                                    0.450943   \nsentence-transformers/all-mpnet-base-v2                                       0.422341   \nsentence-transformers/allenai-specter                                         0.314620   \nsentence-transformers/average_word_embeddings_g...                            0.338484   \nsentence-transformers/average_word_embeddings_k...                            0.339979   \nsentence-transformers/gtr-t5-base                                             0.579747   \nsentence-transformers/gtr-t5-large                                            1.421523   \nsentence-transformers/gtr-t5-xl                                               0.652125   \nsentence-transformers/msmarco-bert-co-condensor                               0.444118   \nsentence-transformers/sentence-t5-large                                       0.684795   \nsentence-transformers/sentence-t5-xl                                          0.512316   \nthenlper/gte-base                                                             0.452752   \nthenlper/gte-large                                                            0.467944   \n\nmodel_2                                             sentence-transformers/gtr-t5-xl  \\\nmodel_1                                                                               \nBAAI/bge-base-en-v1.5                                                      0.446726   \nGritLM/GritLM-7B                                                           0.513704   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                 0.056857   \nNousResearch/Llama-2-7b-hf                                                 0.471243   \nSalesforce/SFR-Embedding-Mistral                                           0.573165   \nSmartComponents/bge-micro-v2                                               0.432264   \nTaylorAI/gte-tiny                                                          0.422918   \nWhereIsAI/UAE-Large-V1                                                     0.457000   \navsolatorio/GIST-Embedding-v0                                              0.458153   \ncroissantllm/CroissantCool                                                 0.438179   \ncroissantllm/CroissantLLMBase                                              0.433211   \ncroissantllm/base_100k                                                     0.438657   \ncroissantllm/base_150k                                                     0.438561   \ncroissantllm/base_50k                                                      0.441695   \ncroissantllm/base_5k                                                       0.425483   \ngoogle/gemma-2b                                                            0.431343   \ngoogle/gemma-2b-it                                                         0.459417   \ngoogle/gemma-7b                                                            0.442751   \ngoogle/gemma-7b-it                                                         0.488423   \ninfgrad/stella-base-en-v2                                                  0.472363   \nintfloat/e5-large-v2                                                       0.503090   \nintfloat/e5-small                                                          0.436824   \nintfloat/multilingual-e5-small                                             0.438195   \nizhx/udever-bloom-560m                                                     0.420568   \njamesgpt1/sf_model_e5                                                      0.450485   \njspringer/echo-mistral-7b-instruct-lasttoken                               0.519805   \nllmrails/ember-v1                                                          0.466552   \nprinceton-nlp/sup-simcse-bert-base-uncased                                 0.405260   \nsentence-transformers/LaBSE                                                0.397016   \nsentence-transformers/all-MiniLM-L6-v2                                     0.405454   \nsentence-transformers/all-distilroberta-v1                                 0.432017   \nsentence-transformers/all-mpnet-base-v2                                    0.434547   \nsentence-transformers/allenai-specter                                      0.316957   \nsentence-transformers/average_word_embeddings_g...                         0.331774   \nsentence-transformers/average_word_embeddings_k...                         0.342870   \nsentence-transformers/gtr-t5-base                                          0.534174   \nsentence-transformers/gtr-t5-large                                         0.655934   \nsentence-transformers/gtr-t5-xl                                            1.418717   \nsentence-transformers/msmarco-bert-co-condensor                            0.451855   \nsentence-transformers/sentence-t5-large                                    0.523900   \nsentence-transformers/sentence-t5-xl                                       0.626466   \nthenlper/gte-base                                                          0.458957   \nthenlper/gte-large                                                         0.460346   \n\nmodel_2                                             sentence-transformers/msmarco-bert-co-condensor  \\\nmodel_1                                                                                               \nBAAI/bge-base-en-v1.5                                                                      0.527686   \nGritLM/GritLM-7B                                                                           0.508990   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                                 0.060608   \nNousResearch/Llama-2-7b-hf                                                                 0.472029   \nSalesforce/SFR-Embedding-Mistral                                                           0.526496   \nSmartComponents/bge-micro-v2                                                               0.480542   \nTaylorAI/gte-tiny                                                                          0.461597   \nWhereIsAI/UAE-Large-V1                                                                     0.494160   \navsolatorio/GIST-Embedding-v0                                                              0.513908   \ncroissantllm/CroissantCool                                                                 0.456904   \ncroissantllm/CroissantLLMBase                                                              0.456770   \ncroissantllm/base_100k                                                                     0.458285   \ncroissantllm/base_150k                                                                     0.452388   \ncroissantllm/base_50k                                                                      0.456349   \ncroissantllm/base_5k                                                                       0.450144   \ngoogle/gemma-2b                                                                            0.445828   \ngoogle/gemma-2b-it                                                                         0.468409   \ngoogle/gemma-7b                                                                            0.442491   \ngoogle/gemma-7b-it                                                                         0.490284   \ninfgrad/stella-base-en-v2                                                                  0.543822   \nintfloat/e5-large-v2                                                                       0.513814   \nintfloat/e5-small                                                                          0.475295   \nintfloat/multilingual-e5-small                                                             0.454372   \nizhx/udever-bloom-560m                                                                     0.446009   \njamesgpt1/sf_model_e5                                                                      0.486706   \njspringer/echo-mistral-7b-instruct-lasttoken                                               0.531873   \nllmrails/ember-v1                                                                          0.490710   \nprinceton-nlp/sup-simcse-bert-base-uncased                                                 0.496090   \nsentence-transformers/LaBSE                                                                0.430592   \nsentence-transformers/all-MiniLM-L6-v2                                                     0.473870   \nsentence-transformers/all-distilroberta-v1                                                 0.472908   \nsentence-transformers/all-mpnet-base-v2                                                    0.462525   \nsentence-transformers/allenai-specter                                                      0.350832   \nsentence-transformers/average_word_embeddings_g...                                         0.381070   \nsentence-transformers/average_word_embeddings_k...                                         0.392059   \nsentence-transformers/gtr-t5-base                                                          0.502281   \nsentence-transformers/gtr-t5-large                                                         0.494595   \nsentence-transformers/gtr-t5-xl                                                            0.507393   \nsentence-transformers/msmarco-bert-co-condensor                                            1.412777   \nsentence-transformers/sentence-t5-large                                                    0.454242   \nsentence-transformers/sentence-t5-xl                                                       0.446312   \nthenlper/gte-base                                                                          0.526197   \nthenlper/gte-large                                                                         0.501386   \n\nmodel_2                                             sentence-transformers/sentence-t5-large  \\\nmodel_1                                                                                       \nBAAI/bge-base-en-v1.5                                                              0.550271   \nGritLM/GritLM-7B                                                                   0.594949   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                         0.065630   \nNousResearch/Llama-2-7b-hf                                                         0.552098   \nSalesforce/SFR-Embedding-Mistral                                                   0.640687   \nSmartComponents/bge-micro-v2                                                       0.492247   \nTaylorAI/gte-tiny                                                                  0.491881   \nWhereIsAI/UAE-Large-V1                                                             0.573612   \navsolatorio/GIST-Embedding-v0                                                      0.561671   \ncroissantllm/CroissantCool                                                         0.525172   \ncroissantllm/CroissantLLMBase                                                      0.523189   \ncroissantllm/base_100k                                                             0.511788   \ncroissantllm/base_150k                                                             0.517542   \ncroissantllm/base_50k                                                              0.526815   \ncroissantllm/base_5k                                                               0.485683   \ngoogle/gemma-2b                                                                    0.515956   \ngoogle/gemma-2b-it                                                                 0.543973   \ngoogle/gemma-7b                                                                    0.515464   \ngoogle/gemma-7b-it                                                                 0.569164   \ninfgrad/stella-base-en-v2                                                          0.574318   \nintfloat/e5-large-v2                                                               0.600376   \nintfloat/e5-small                                                                  0.517320   \nintfloat/multilingual-e5-small                                                     0.516583   \nizhx/udever-bloom-560m                                                             0.488253   \njamesgpt1/sf_model_e5                                                              0.569833   \njspringer/echo-mistral-7b-instruct-lasttoken                                       0.608086   \nllmrails/ember-v1                                                                  0.574385   \nprinceton-nlp/sup-simcse-bert-base-uncased                                         0.477122   \nsentence-transformers/LaBSE                                                        0.467133   \nsentence-transformers/all-MiniLM-L6-v2                                             0.481414   \nsentence-transformers/all-distilroberta-v1                                         0.528093   \nsentence-transformers/all-mpnet-base-v2                                            0.519832   \nsentence-transformers/allenai-specter                                              0.365971   \nsentence-transformers/average_word_embeddings_g...                                 0.395086   \nsentence-transformers/average_word_embeddings_k...                                 0.396114   \nsentence-transformers/gtr-t5-base                                                  0.569183   \nsentence-transformers/gtr-t5-large                                                 0.748229   \nsentence-transformers/gtr-t5-xl                                                    0.610276   \nsentence-transformers/msmarco-bert-co-condensor                                    0.495614   \nsentence-transformers/sentence-t5-large                                            1.397966   \nsentence-transformers/sentence-t5-xl                                               0.724738   \nthenlper/gte-base                                                                  0.562936   \nthenlper/gte-large                                                                 0.569840   \n\nmodel_2                                             sentence-transformers/sentence-t5-xl  \\\nmodel_1                                                                                    \nBAAI/bge-base-en-v1.5                                                           0.515953   \nGritLM/GritLM-7B                                                                0.564597   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                                      0.060249   \nNousResearch/Llama-2-7b-hf                                                      0.534061   \nSalesforce/SFR-Embedding-Mistral                                                0.632684   \nSmartComponents/bge-micro-v2                                                    0.442565   \nTaylorAI/gte-tiny                                                               0.461073   \nWhereIsAI/UAE-Large-V1                                                          0.515566   \navsolatorio/GIST-Embedding-v0                                                   0.514599   \ncroissantllm/CroissantCool                                                      0.487484   \ncroissantllm/CroissantLLMBase                                                   0.497127   \ncroissantllm/base_100k                                                          0.494208   \ncroissantllm/base_150k                                                          0.489188   \ncroissantllm/base_50k                                                           0.489695   \ncroissantllm/base_5k                                                            0.463501   \ngoogle/gemma-2b                                                                 0.503646   \ngoogle/gemma-2b-it                                                              0.518715   \ngoogle/gemma-7b                                                                 0.506848   \ngoogle/gemma-7b-it                                                              0.547986   \ninfgrad/stella-base-en-v2                                                       0.528276   \nintfloat/e5-large-v2                                                            0.551443   \nintfloat/e5-small                                                               0.467882   \nintfloat/multilingual-e5-small                                                  0.461564   \nizhx/udever-bloom-560m                                                          0.448887   \njamesgpt1/sf_model_e5                                                           0.535991   \njspringer/echo-mistral-7b-instruct-lasttoken                                    0.589319   \nllmrails/ember-v1                                                               0.524408   \nprinceton-nlp/sup-simcse-bert-base-uncased                                      0.462614   \nsentence-transformers/LaBSE                                                     0.430746   \nsentence-transformers/all-MiniLM-L6-v2                                          0.451281   \nsentence-transformers/all-distilroberta-v1                                      0.475899   \nsentence-transformers/all-mpnet-base-v2                                         0.473908   \nsentence-transformers/allenai-specter                                           0.341997   \nsentence-transformers/average_word_embeddings_g...                              0.365068   \nsentence-transformers/average_word_embeddings_k...                              0.378995   \nsentence-transformers/gtr-t5-base                                               0.513009   \nsentence-transformers/gtr-t5-large                                              0.579101   \nsentence-transformers/gtr-t5-xl                                                 0.666684   \nsentence-transformers/msmarco-bert-co-condensor                                 0.459660   \nsentence-transformers/sentence-t5-large                                         0.692518   \nsentence-transformers/sentence-t5-xl                                            1.407957   \nthenlper/gte-base                                                               0.516471   \nthenlper/gte-large                                                              0.534219   \n\nmodel_2                                             thenlper/gte-base  \\\nmodel_1                                                                 \nBAAI/bge-base-en-v1.5                                        1.076783   \nGritLM/GritLM-7B                                             0.519053   \nHuggingFaceM4/tiny-random-LlamaForCausalLM                   0.061303   \nNousResearch/Llama-2-7b-hf                                   0.488254   \nSalesforce/SFR-Embedding-Mistral                             0.521461   \nSmartComponents/bge-micro-v2                                 0.484923   \nTaylorAI/gte-tiny                                            0.510063   \nWhereIsAI/UAE-Large-V1                                       0.567829   \navsolatorio/GIST-Embedding-v0                                1.154581   \ncroissantllm/CroissantCool                                   0.470531   \ncroissantllm/CroissantLLMBase                                0.464497   \ncroissantllm/base_100k                                       0.451867   \ncroissantllm/base_150k                                       0.470298   \ncroissantllm/base_50k                                        0.466467   \ncroissantllm/base_5k                                         0.452161   \ngoogle/gemma-2b                                              0.457048   \ngoogle/gemma-2b-it                                           0.474204   \ngoogle/gemma-7b                                              0.449998   \ngoogle/gemma-7b-it                                           0.488159   \ninfgrad/stella-base-en-v2                                    0.889806   \nintfloat/e5-large-v2                                         0.536867   \nintfloat/e5-small                                            0.480317   \nintfloat/multilingual-e5-small                               0.459383   \nizhx/udever-bloom-560m                                       0.447860   \njamesgpt1/sf_model_e5                                        0.569318   \njspringer/echo-mistral-7b-instruct-lasttoken                 0.543221   \nllmrails/ember-v1                                            0.577232   \nprinceton-nlp/sup-simcse-bert-base-uncased                   0.472151   \nsentence-transformers/LaBSE                                  0.434410   \nsentence-transformers/all-MiniLM-L6-v2                       0.450123   \nsentence-transformers/all-distilroberta-v1                   0.467871   \nsentence-transformers/all-mpnet-base-v2                      0.462058   \nsentence-transformers/allenai-specter                        0.345012   \nsentence-transformers/average_word_embeddings_g...           0.369846   \nsentence-transformers/average_word_embeddings_k...           0.374786   \nsentence-transformers/gtr-t5-base                            0.477377   \nsentence-transformers/gtr-t5-large                           0.483435   \nsentence-transformers/gtr-t5-xl                              0.480519   \nsentence-transformers/msmarco-bert-co-condensor              0.479899   \nsentence-transformers/sentence-t5-large                      0.491344   \nsentence-transformers/sentence-t5-xl                         0.477136   \nthenlper/gte-base                                            1.423794   \nthenlper/gte-large                                           0.609543   \n\nmodel_2                                             thenlper/gte-large  \nmodel_1                                                                 \nBAAI/bge-base-en-v1.5                                         0.629245  \nGritLM/GritLM-7B                                              0.572204  \nHuggingFaceM4/tiny-random-LlamaForCausalLM                    0.061814  \nNousResearch/Llama-2-7b-hf                                    0.522408  \nSalesforce/SFR-Embedding-Mistral                              0.594409  \nSmartComponents/bge-micro-v2                                  0.507655  \nTaylorAI/gte-tiny                                             0.535302  \nWhereIsAI/UAE-Large-V1                                        1.131927  \navsolatorio/GIST-Embedding-v0                                 0.618793  \ncroissantllm/CroissantCool                                    0.503840  \ncroissantllm/CroissantLLMBase                                 0.501954  \ncroissantllm/base_100k                                        0.498090  \ncroissantllm/base_150k                                        0.499293  \ncroissantllm/base_50k                                         0.503029  \ncroissantllm/base_5k                                          0.493005  \ngoogle/gemma-2b                                               0.506124  \ngoogle/gemma-2b-it                                            0.522266  \ngoogle/gemma-7b                                               0.490493  \ngoogle/gemma-7b-it                                            0.542739  \ninfgrad/stella-base-en-v2                                     0.663747  \nintfloat/e5-large-v2                                          0.594386  \nintfloat/e5-small                                             0.517261  \nintfloat/multilingual-e5-small                                0.494088  \nizhx/udever-bloom-560m                                        0.479816  \njamesgpt1/sf_model_e5                                         1.038521  \njspringer/echo-mistral-7b-instruct-lasttoken                  0.570880  \nllmrails/ember-v1                                             1.044197  \nprinceton-nlp/sup-simcse-bert-base-uncased                    0.500683  \nsentence-transformers/LaBSE                                   0.470597  \nsentence-transformers/all-MiniLM-L6-v2                        0.480975  \nsentence-transformers/all-distilroberta-v1                    0.511429  \nsentence-transformers/all-mpnet-base-v2                       0.488959  \nsentence-transformers/allenai-specter                         0.376054  \nsentence-transformers/average_word_embeddings_g...            0.413476  \nsentence-transformers/average_word_embeddings_k...            0.407064  \nsentence-transformers/gtr-t5-base                             0.477513  \nsentence-transformers/gtr-t5-large                            0.525563  \nsentence-transformers/gtr-t5-xl                               0.530751  \nsentence-transformers/msmarco-bert-co-condensor               0.512684  \nsentence-transformers/sentence-t5-large                       0.538427  \nsentence-transformers/sentence-t5-xl                          0.529059  \nthenlper/gte-base                                             0.642169  \nthenlper/gte-large                                            1.423009  \n\n[43 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>model_2</th>\n      <th>BAAI/bge-base-en-v1.5</th>\n      <th>GritLM/GritLM-7B</th>\n      <th>HuggingFaceM4/tiny-random-LlamaForCausalLM</th>\n      <th>NousResearch/Llama-2-7b-hf</th>\n      <th>Salesforce/SFR-Embedding-Mistral</th>\n      <th>SmartComponents/bge-micro-v2</th>\n      <th>TaylorAI/gte-tiny</th>\n      <th>WhereIsAI/UAE-Large-V1</th>\n      <th>avsolatorio/GIST-Embedding-v0</th>\n      <th>croissantllm/CroissantCool</th>\n      <th>...</th>\n      <th>sentence-transformers/average_word_embeddings_glove.6B.300d</th>\n      <th>sentence-transformers/average_word_embeddings_komninos</th>\n      <th>sentence-transformers/gtr-t5-base</th>\n      <th>sentence-transformers/gtr-t5-large</th>\n      <th>sentence-transformers/gtr-t5-xl</th>\n      <th>sentence-transformers/msmarco-bert-co-condensor</th>\n      <th>sentence-transformers/sentence-t5-large</th>\n      <th>sentence-transformers/sentence-t5-xl</th>\n      <th>thenlper/gte-base</th>\n      <th>thenlper/gte-large</th>\n    </tr>\n    <tr>\n      <th>model_1</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BAAI/bge-base-en-v1.5</th>\n      <td>1.408755</td>\n      <td>0.556619</td>\n      <td>0.216323</td>\n      <td>0.658130</td>\n      <td>0.627063</td>\n      <td>0.505610</td>\n      <td>0.518470</td>\n      <td>0.672778</td>\n      <td>1.279503</td>\n      <td>0.580892</td>\n      <td>...</td>\n      <td>0.498936</td>\n      <td>0.445808</td>\n      <td>0.476813</td>\n      <td>0.465046</td>\n      <td>0.446726</td>\n      <td>0.527686</td>\n      <td>0.550271</td>\n      <td>0.515953</td>\n      <td>1.076783</td>\n      <td>0.629245</td>\n    </tr>\n    <tr>\n      <th>GritLM/GritLM-7B</th>\n      <td>0.562268</td>\n      <td>0.928033</td>\n      <td>0.370918</td>\n      <td>0.866750</td>\n      <td>0.665229</td>\n      <td>0.512743</td>\n      <td>0.502282</td>\n      <td>0.613329</td>\n      <td>0.591918</td>\n      <td>0.780834</td>\n      <td>...</td>\n      <td>0.556971</td>\n      <td>0.537035</td>\n      <td>0.506658</td>\n      <td>0.514760</td>\n      <td>0.513704</td>\n      <td>0.508990</td>\n      <td>0.594949</td>\n      <td>0.564597</td>\n      <td>0.519053</td>\n      <td>0.572204</td>\n    </tr>\n    <tr>\n      <th>HuggingFaceM4/tiny-random-LlamaForCausalLM</th>\n      <td>0.073016</td>\n      <td>0.063781</td>\n      <td>1.128528</td>\n      <td>0.089295</td>\n      <td>0.033481</td>\n      <td>0.071567</td>\n      <td>0.069533</td>\n      <td>0.067744</td>\n      <td>0.069579</td>\n      <td>0.089979</td>\n      <td>...</td>\n      <td>0.115742</td>\n      <td>0.105286</td>\n      <td>0.056679</td>\n      <td>0.058115</td>\n      <td>0.056857</td>\n      <td>0.060608</td>\n      <td>0.065630</td>\n      <td>0.060249</td>\n      <td>0.061303</td>\n      <td>0.061814</td>\n    </tr>\n    <tr>\n      <th>NousResearch/Llama-2-7b-hf</th>\n      <td>0.523777</td>\n      <td>0.692050</td>\n      <td>0.364495</td>\n      <td>1.022687</td>\n      <td>0.548256</td>\n      <td>0.475209</td>\n      <td>0.462156</td>\n      <td>0.576038</td>\n      <td>0.546495</td>\n      <td>0.759894</td>\n      <td>...</td>\n      <td>0.533015</td>\n      <td>0.499705</td>\n      <td>0.464317</td>\n      <td>0.459331</td>\n      <td>0.471243</td>\n      <td>0.472029</td>\n      <td>0.552098</td>\n      <td>0.534061</td>\n      <td>0.488254</td>\n      <td>0.522408</td>\n    </tr>\n    <tr>\n      <th>Salesforce/SFR-Embedding-Mistral</th>\n      <td>0.583908</td>\n      <td>0.644836</td>\n      <td>0.298796</td>\n      <td>0.729444</td>\n      <td>1.141443</td>\n      <td>0.541588</td>\n      <td>0.532534</td>\n      <td>0.648821</td>\n      <td>0.612515</td>\n      <td>0.638281</td>\n      <td>...</td>\n      <td>0.540469</td>\n      <td>0.499941</td>\n      <td>0.548169</td>\n      <td>0.555192</td>\n      <td>0.573165</td>\n      <td>0.526496</td>\n      <td>0.640687</td>\n      <td>0.632684</td>\n      <td>0.521461</td>\n      <td>0.594409</td>\n    </tr>\n    <tr>\n      <th>SmartComponents/bge-micro-v2</th>\n      <td>0.511468</td>\n      <td>0.547267</td>\n      <td>0.206421</td>\n      <td>0.635582</td>\n      <td>0.620145</td>\n      <td>1.410436</td>\n      <td>0.798591</td>\n      <td>0.553717</td>\n      <td>0.549635</td>\n      <td>0.571141</td>\n      <td>...</td>\n      <td>0.520413</td>\n      <td>0.468280</td>\n      <td>0.443943</td>\n      <td>0.413541</td>\n      <td>0.432264</td>\n      <td>0.480542</td>\n      <td>0.492247</td>\n      <td>0.442565</td>\n      <td>0.484923</td>\n      <td>0.507655</td>\n    </tr>\n    <tr>\n      <th>TaylorAI/gte-tiny</th>\n      <td>0.547937</td>\n      <td>0.548944</td>\n      <td>0.198077</td>\n      <td>0.640014</td>\n      <td>0.607331</td>\n      <td>0.806125</td>\n      <td>1.424008</td>\n      <td>0.563739</td>\n      <td>0.574487</td>\n      <td>0.578049</td>\n      <td>...</td>\n      <td>0.503811</td>\n      <td>0.460555</td>\n      <td>0.442818</td>\n      <td>0.413936</td>\n      <td>0.422918</td>\n      <td>0.461597</td>\n      <td>0.491881</td>\n      <td>0.461073</td>\n      <td>0.510063</td>\n      <td>0.535302</td>\n    </tr>\n    <tr>\n      <th>WhereIsAI/UAE-Large-V1</th>\n      <td>0.625907</td>\n      <td>0.556661</td>\n      <td>0.199992</td>\n      <td>0.634317</td>\n      <td>0.646123</td>\n      <td>0.501665</td>\n      <td>0.507954</td>\n      <td>1.413947</td>\n      <td>0.667389</td>\n      <td>0.588979</td>\n      <td>...</td>\n      <td>0.488767</td>\n      <td>0.440581</td>\n      <td>0.465090</td>\n      <td>0.460625</td>\n      <td>0.457000</td>\n      <td>0.494160</td>\n      <td>0.573612</td>\n      <td>0.515566</td>\n      <td>0.567829</td>\n      <td>1.131927</td>\n    </tr>\n    <tr>\n      <th>avsolatorio/GIST-Embedding-v0</th>\n      <td>1.252645</td>\n      <td>0.559947</td>\n      <td>0.207721</td>\n      <td>0.655228</td>\n      <td>0.630045</td>\n      <td>0.524688</td>\n      <td>0.532512</td>\n      <td>0.680945</td>\n      <td>1.427954</td>\n      <td>0.588988</td>\n      <td>...</td>\n      <td>0.494188</td>\n      <td>0.449655</td>\n      <td>0.459249</td>\n      <td>0.451516</td>\n      <td>0.458153</td>\n      <td>0.513908</td>\n      <td>0.561671</td>\n      <td>0.514599</td>\n      <td>1.154581</td>\n      <td>0.618793</td>\n    </tr>\n    <tr>\n      <th>croissantllm/CroissantCool</th>\n      <td>0.510853</td>\n      <td>0.671090</td>\n      <td>0.314416</td>\n      <td>0.798290</td>\n      <td>0.543304</td>\n      <td>0.455690</td>\n      <td>0.437453</td>\n      <td>0.560995</td>\n      <td>0.528483</td>\n      <td>1.254203</td>\n      <td>...</td>\n      <td>0.505237</td>\n      <td>0.473526</td>\n      <td>0.454472</td>\n      <td>0.438180</td>\n      <td>0.438179</td>\n      <td>0.456904</td>\n      <td>0.525172</td>\n      <td>0.487484</td>\n      <td>0.470531</td>\n      <td>0.503840</td>\n    </tr>\n    <tr>\n      <th>croissantllm/CroissantLLMBase</th>\n      <td>0.511958</td>\n      <td>0.663391</td>\n      <td>0.316978</td>\n      <td>0.797183</td>\n      <td>0.525400</td>\n      <td>0.457884</td>\n      <td>0.443562</td>\n      <td>0.552638</td>\n      <td>0.529074</td>\n      <td>0.975226</td>\n      <td>...</td>\n      <td>0.505339</td>\n      <td>0.474057</td>\n      <td>0.446682</td>\n      <td>0.441439</td>\n      <td>0.433211</td>\n      <td>0.456770</td>\n      <td>0.523189</td>\n      <td>0.497127</td>\n      <td>0.464497</td>\n      <td>0.501954</td>\n    </tr>\n    <tr>\n      <th>croissantllm/base_100k</th>\n      <td>0.511069</td>\n      <td>0.671474</td>\n      <td>0.315959</td>\n      <td>0.797092</td>\n      <td>0.525701</td>\n      <td>0.457018</td>\n      <td>0.445600</td>\n      <td>0.559053</td>\n      <td>0.514522</td>\n      <td>0.917230</td>\n      <td>...</td>\n      <td>0.505628</td>\n      <td>0.478196</td>\n      <td>0.444599</td>\n      <td>0.450415</td>\n      <td>0.438657</td>\n      <td>0.458285</td>\n      <td>0.511788</td>\n      <td>0.494208</td>\n      <td>0.451867</td>\n      <td>0.498090</td>\n    </tr>\n    <tr>\n      <th>croissantllm/base_150k</th>\n      <td>0.506839</td>\n      <td>0.648802</td>\n      <td>0.318091</td>\n      <td>0.795367</td>\n      <td>0.540232</td>\n      <td>0.463220</td>\n      <td>0.440017</td>\n      <td>0.552501</td>\n      <td>0.526479</td>\n      <td>0.957526</td>\n      <td>...</td>\n      <td>0.513175</td>\n      <td>0.470818</td>\n      <td>0.445761</td>\n      <td>0.451087</td>\n      <td>0.438561</td>\n      <td>0.452388</td>\n      <td>0.517542</td>\n      <td>0.489188</td>\n      <td>0.470298</td>\n      <td>0.499293</td>\n    </tr>\n    <tr>\n      <th>croissantllm/base_50k</th>\n      <td>0.504344</td>\n      <td>0.677135</td>\n      <td>0.319642</td>\n      <td>0.796211</td>\n      <td>0.540682</td>\n      <td>0.461757</td>\n      <td>0.446798</td>\n      <td>0.554349</td>\n      <td>0.534027</td>\n      <td>0.859379</td>\n      <td>...</td>\n      <td>0.510845</td>\n      <td>0.486585</td>\n      <td>0.456624</td>\n      <td>0.444806</td>\n      <td>0.441695</td>\n      <td>0.456349</td>\n      <td>0.526815</td>\n      <td>0.489695</td>\n      <td>0.466467</td>\n      <td>0.503029</td>\n    </tr>\n    <tr>\n      <th>croissantllm/base_5k</th>\n      <td>0.501560</td>\n      <td>0.630032</td>\n      <td>0.328694</td>\n      <td>0.738761</td>\n      <td>0.506305</td>\n      <td>0.464391</td>\n      <td>0.446950</td>\n      <td>0.529942</td>\n      <td>0.510975</td>\n      <td>0.756661</td>\n      <td>...</td>\n      <td>0.523563</td>\n      <td>0.489645</td>\n      <td>0.449824</td>\n      <td>0.424147</td>\n      <td>0.425483</td>\n      <td>0.450144</td>\n      <td>0.485683</td>\n      <td>0.463501</td>\n      <td>0.452161</td>\n      <td>0.493005</td>\n    </tr>\n    <tr>\n      <th>google/gemma-2b</th>\n      <td>0.497651</td>\n      <td>0.685574</td>\n      <td>0.305570</td>\n      <td>0.819559</td>\n      <td>0.544654</td>\n      <td>0.448804</td>\n      <td>0.426255</td>\n      <td>0.539649</td>\n      <td>0.516216</td>\n      <td>0.728332</td>\n      <td>...</td>\n      <td>0.495073</td>\n      <td>0.468358</td>\n      <td>0.442275</td>\n      <td>0.436050</td>\n      <td>0.431343</td>\n      <td>0.445828</td>\n      <td>0.515956</td>\n      <td>0.503646</td>\n      <td>0.457048</td>\n      <td>0.506124</td>\n    </tr>\n    <tr>\n      <th>google/gemma-2b-it</th>\n      <td>0.522912</td>\n      <td>0.698448</td>\n      <td>0.313725</td>\n      <td>0.797754</td>\n      <td>0.597168</td>\n      <td>0.474761</td>\n      <td>0.450217</td>\n      <td>0.576367</td>\n      <td>0.552545</td>\n      <td>0.753346</td>\n      <td>...</td>\n      <td>0.525578</td>\n      <td>0.492195</td>\n      <td>0.459755</td>\n      <td>0.461056</td>\n      <td>0.459417</td>\n      <td>0.468409</td>\n      <td>0.543973</td>\n      <td>0.518715</td>\n      <td>0.474204</td>\n      <td>0.522266</td>\n    </tr>\n    <tr>\n      <th>google/gemma-7b</th>\n      <td>0.489064</td>\n      <td>0.650991</td>\n      <td>0.316679</td>\n      <td>0.771033</td>\n      <td>0.443719</td>\n      <td>0.439864</td>\n      <td>0.429306</td>\n      <td>0.526452</td>\n      <td>0.511481</td>\n      <td>0.701325</td>\n      <td>...</td>\n      <td>0.493149</td>\n      <td>0.454896</td>\n      <td>0.431446</td>\n      <td>0.434091</td>\n      <td>0.442751</td>\n      <td>0.442491</td>\n      <td>0.515464</td>\n      <td>0.506848</td>\n      <td>0.449998</td>\n      <td>0.490493</td>\n    </tr>\n    <tr>\n      <th>google/gemma-7b-it</th>\n      <td>0.534298</td>\n      <td>0.705515</td>\n      <td>0.338456</td>\n      <td>0.816301</td>\n      <td>0.602809</td>\n      <td>0.491458</td>\n      <td>0.463196</td>\n      <td>0.595353</td>\n      <td>0.563139</td>\n      <td>0.747503</td>\n      <td>...</td>\n      <td>0.526362</td>\n      <td>0.504533</td>\n      <td>0.488652</td>\n      <td>0.483912</td>\n      <td>0.488423</td>\n      <td>0.490284</td>\n      <td>0.569164</td>\n      <td>0.547986</td>\n      <td>0.488159</td>\n      <td>0.542739</td>\n    </tr>\n    <tr>\n      <th>infgrad/stella-base-en-v2</th>\n      <td>0.996816</td>\n      <td>0.573110</td>\n      <td>0.223171</td>\n      <td>0.651793</td>\n      <td>0.639125</td>\n      <td>0.555202</td>\n      <td>0.554735</td>\n      <td>0.729123</td>\n      <td>0.998513</td>\n      <td>0.602653</td>\n      <td>...</td>\n      <td>0.518842</td>\n      <td>0.470766</td>\n      <td>0.492605</td>\n      <td>0.487023</td>\n      <td>0.472363</td>\n      <td>0.543822</td>\n      <td>0.574318</td>\n      <td>0.528276</td>\n      <td>0.889806</td>\n      <td>0.663747</td>\n    </tr>\n    <tr>\n      <th>intfloat/e5-large-v2</th>\n      <td>0.588100</td>\n      <td>0.600249</td>\n      <td>0.217827</td>\n      <td>0.687747</td>\n      <td>0.693649</td>\n      <td>0.503315</td>\n      <td>0.513624</td>\n      <td>0.630756</td>\n      <td>0.605085</td>\n      <td>0.607066</td>\n      <td>...</td>\n      <td>0.506488</td>\n      <td>0.460488</td>\n      <td>0.499514</td>\n      <td>0.504611</td>\n      <td>0.503090</td>\n      <td>0.513814</td>\n      <td>0.600376</td>\n      <td>0.551443</td>\n      <td>0.536867</td>\n      <td>0.594386</td>\n    </tr>\n    <tr>\n      <th>intfloat/e5-small</th>\n      <td>0.531051</td>\n      <td>0.567639</td>\n      <td>0.192635</td>\n      <td>0.648732</td>\n      <td>0.605417</td>\n      <td>0.510709</td>\n      <td>0.503236</td>\n      <td>0.558432</td>\n      <td>0.548294</td>\n      <td>0.577086</td>\n      <td>...</td>\n      <td>0.487990</td>\n      <td>0.442540</td>\n      <td>0.461136</td>\n      <td>0.446859</td>\n      <td>0.436824</td>\n      <td>0.475295</td>\n      <td>0.517320</td>\n      <td>0.467882</td>\n      <td>0.480317</td>\n      <td>0.517261</td>\n    </tr>\n    <tr>\n      <th>intfloat/multilingual-e5-small</th>\n      <td>0.524799</td>\n      <td>0.555619</td>\n      <td>0.202738</td>\n      <td>0.642422</td>\n      <td>0.647128</td>\n      <td>0.498778</td>\n      <td>0.489159</td>\n      <td>0.561169</td>\n      <td>0.526664</td>\n      <td>0.577948</td>\n      <td>...</td>\n      <td>0.484153</td>\n      <td>0.452542</td>\n      <td>0.470881</td>\n      <td>0.446955</td>\n      <td>0.438195</td>\n      <td>0.454372</td>\n      <td>0.516583</td>\n      <td>0.461564</td>\n      <td>0.459383</td>\n      <td>0.494088</td>\n    </tr>\n    <tr>\n      <th>izhx/udever-bloom-560m</th>\n      <td>0.489196</td>\n      <td>0.618040</td>\n      <td>0.292172</td>\n      <td>0.710372</td>\n      <td>0.550806</td>\n      <td>0.466338</td>\n      <td>0.452572</td>\n      <td>0.523653</td>\n      <td>0.507423</td>\n      <td>0.680482</td>\n      <td>...</td>\n      <td>0.514319</td>\n      <td>0.482460</td>\n      <td>0.436873</td>\n      <td>0.419580</td>\n      <td>0.420568</td>\n      <td>0.446009</td>\n      <td>0.488253</td>\n      <td>0.448887</td>\n      <td>0.447860</td>\n      <td>0.479816</td>\n    </tr>\n    <tr>\n      <th>jamesgpt1/sf_model_e5</th>\n      <td>0.611252</td>\n      <td>0.567582</td>\n      <td>0.211733</td>\n      <td>0.647158</td>\n      <td>0.632201</td>\n      <td>0.508962</td>\n      <td>0.502087</td>\n      <td>1.243419</td>\n      <td>0.647557</td>\n      <td>0.590236</td>\n      <td>...</td>\n      <td>0.479518</td>\n      <td>0.444600</td>\n      <td>0.458527</td>\n      <td>0.476782</td>\n      <td>0.450485</td>\n      <td>0.486706</td>\n      <td>0.569833</td>\n      <td>0.535991</td>\n      <td>0.569318</td>\n      <td>1.038521</td>\n    </tr>\n    <tr>\n      <th>jspringer/echo-mistral-7b-instruct-lasttoken</th>\n      <td>0.579410</td>\n      <td>0.798141</td>\n      <td>0.384352</td>\n      <td>0.862959</td>\n      <td>0.718967</td>\n      <td>0.534985</td>\n      <td>0.521874</td>\n      <td>0.634924</td>\n      <td>0.598169</td>\n      <td>0.785881</td>\n      <td>...</td>\n      <td>0.580162</td>\n      <td>0.533257</td>\n      <td>0.535313</td>\n      <td>0.513519</td>\n      <td>0.519805</td>\n      <td>0.531873</td>\n      <td>0.608086</td>\n      <td>0.589319</td>\n      <td>0.543221</td>\n      <td>0.570880</td>\n    </tr>\n    <tr>\n      <th>llmrails/ember-v1</th>\n      <td>0.622414</td>\n      <td>0.577960</td>\n      <td>0.216342</td>\n      <td>0.662653</td>\n      <td>0.628820</td>\n      <td>0.505790</td>\n      <td>0.504016</td>\n      <td>1.226934</td>\n      <td>0.658961</td>\n      <td>0.584525</td>\n      <td>...</td>\n      <td>0.488522</td>\n      <td>0.449348</td>\n      <td>0.463220</td>\n      <td>0.462539</td>\n      <td>0.466552</td>\n      <td>0.490710</td>\n      <td>0.574385</td>\n      <td>0.524408</td>\n      <td>0.577232</td>\n      <td>1.044197</td>\n    </tr>\n    <tr>\n      <th>princeton-nlp/sup-simcse-bert-base-uncased</th>\n      <td>0.526177</td>\n      <td>0.541301</td>\n      <td>0.250812</td>\n      <td>0.630290</td>\n      <td>0.565965</td>\n      <td>0.474583</td>\n      <td>0.448323</td>\n      <td>0.538649</td>\n      <td>0.547264</td>\n      <td>0.586227</td>\n      <td>...</td>\n      <td>0.518778</td>\n      <td>0.481821</td>\n      <td>0.418486</td>\n      <td>0.408098</td>\n      <td>0.405260</td>\n      <td>0.496090</td>\n      <td>0.477122</td>\n      <td>0.462614</td>\n      <td>0.472151</td>\n      <td>0.500683</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/LaBSE</th>\n      <td>0.466823</td>\n      <td>0.545317</td>\n      <td>0.225060</td>\n      <td>0.651559</td>\n      <td>0.542894</td>\n      <td>0.456566</td>\n      <td>0.433137</td>\n      <td>0.505085</td>\n      <td>0.479587</td>\n      <td>0.595097</td>\n      <td>...</td>\n      <td>0.496056</td>\n      <td>0.456030</td>\n      <td>0.403749</td>\n      <td>0.400974</td>\n      <td>0.397016</td>\n      <td>0.430592</td>\n      <td>0.467133</td>\n      <td>0.430746</td>\n      <td>0.434410</td>\n      <td>0.470597</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/all-MiniLM-L6-v2</th>\n      <td>0.504232</td>\n      <td>0.525694</td>\n      <td>0.184804</td>\n      <td>0.624404</td>\n      <td>0.604181</td>\n      <td>0.507903</td>\n      <td>0.489828</td>\n      <td>0.530816</td>\n      <td>0.525732</td>\n      <td>0.539742</td>\n      <td>...</td>\n      <td>0.466498</td>\n      <td>0.420341</td>\n      <td>0.418989</td>\n      <td>0.416894</td>\n      <td>0.405454</td>\n      <td>0.473870</td>\n      <td>0.481414</td>\n      <td>0.451281</td>\n      <td>0.450123</td>\n      <td>0.480975</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/all-distilroberta-v1</th>\n      <td>0.503751</td>\n      <td>0.558081</td>\n      <td>0.199878</td>\n      <td>0.630000</td>\n      <td>0.599850</td>\n      <td>0.464963</td>\n      <td>0.454726</td>\n      <td>0.560070</td>\n      <td>0.546914</td>\n      <td>0.566146</td>\n      <td>...</td>\n      <td>0.464665</td>\n      <td>0.420933</td>\n      <td>0.464351</td>\n      <td>0.450943</td>\n      <td>0.432017</td>\n      <td>0.472908</td>\n      <td>0.528093</td>\n      <td>0.475899</td>\n      <td>0.467871</td>\n      <td>0.511429</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/all-mpnet-base-v2</th>\n      <td>0.501740</td>\n      <td>0.534215</td>\n      <td>0.181181</td>\n      <td>0.611461</td>\n      <td>0.617648</td>\n      <td>0.436941</td>\n      <td>0.431897</td>\n      <td>0.547799</td>\n      <td>0.531392</td>\n      <td>0.549363</td>\n      <td>...</td>\n      <td>0.441049</td>\n      <td>0.396499</td>\n      <td>0.433293</td>\n      <td>0.422341</td>\n      <td>0.434547</td>\n      <td>0.462525</td>\n      <td>0.519832</td>\n      <td>0.473908</td>\n      <td>0.462058</td>\n      <td>0.488959</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/allenai-specter</th>\n      <td>0.379962</td>\n      <td>0.434403</td>\n      <td>0.201173</td>\n      <td>0.490597</td>\n      <td>0.413534</td>\n      <td>0.364405</td>\n      <td>0.344119</td>\n      <td>0.412861</td>\n      <td>0.393944</td>\n      <td>0.461949</td>\n      <td>...</td>\n      <td>0.404639</td>\n      <td>0.370386</td>\n      <td>0.330406</td>\n      <td>0.314620</td>\n      <td>0.316957</td>\n      <td>0.350832</td>\n      <td>0.365971</td>\n      <td>0.341997</td>\n      <td>0.345012</td>\n      <td>0.376054</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/average_word_embeddings_glove.6B.300d</th>\n      <td>0.413889</td>\n      <td>0.462658</td>\n      <td>0.138756</td>\n      <td>0.538345</td>\n      <td>0.467645</td>\n      <td>0.400742</td>\n      <td>0.379588</td>\n      <td>0.448311</td>\n      <td>0.427350</td>\n      <td>0.496353</td>\n      <td>...</td>\n      <td>1.223311</td>\n      <td>0.653541</td>\n      <td>0.352274</td>\n      <td>0.338484</td>\n      <td>0.331774</td>\n      <td>0.381070</td>\n      <td>0.395086</td>\n      <td>0.365068</td>\n      <td>0.369846</td>\n      <td>0.413476</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/average_word_embeddings_komninos</th>\n      <td>0.410048</td>\n      <td>0.469406</td>\n      <td>0.150245</td>\n      <td>0.549655</td>\n      <td>0.500383</td>\n      <td>0.406297</td>\n      <td>0.381755</td>\n      <td>0.452204</td>\n      <td>0.435028</td>\n      <td>0.496225</td>\n      <td>...</td>\n      <td>0.749655</td>\n      <td>1.234867</td>\n      <td>0.358267</td>\n      <td>0.339979</td>\n      <td>0.342870</td>\n      <td>0.392059</td>\n      <td>0.396114</td>\n      <td>0.378995</td>\n      <td>0.374786</td>\n      <td>0.407064</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/gtr-t5-base</th>\n      <td>0.521647</td>\n      <td>0.544015</td>\n      <td>0.204846</td>\n      <td>0.659980</td>\n      <td>0.657463</td>\n      <td>0.482473</td>\n      <td>0.454860</td>\n      <td>0.574804</td>\n      <td>0.528916</td>\n      <td>0.597972</td>\n      <td>...</td>\n      <td>0.482651</td>\n      <td>0.444268</td>\n      <td>1.423988</td>\n      <td>0.579747</td>\n      <td>0.534174</td>\n      <td>0.502281</td>\n      <td>0.569183</td>\n      <td>0.513009</td>\n      <td>0.477377</td>\n      <td>0.477513</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/gtr-t5-large</th>\n      <td>0.533506</td>\n      <td>0.593557</td>\n      <td>0.208719</td>\n      <td>0.667555</td>\n      <td>0.672884</td>\n      <td>0.478711</td>\n      <td>0.473222</td>\n      <td>0.587680</td>\n      <td>0.541225</td>\n      <td>0.597531</td>\n      <td>...</td>\n      <td>0.495393</td>\n      <td>0.452145</td>\n      <td>0.638125</td>\n      <td>1.421523</td>\n      <td>0.655934</td>\n      <td>0.494595</td>\n      <td>0.748229</td>\n      <td>0.579101</td>\n      <td>0.483435</td>\n      <td>0.525563</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/gtr-t5-xl</th>\n      <td>0.515018</td>\n      <td>0.577901</td>\n      <td>0.204779</td>\n      <td>0.680401</td>\n      <td>0.685843</td>\n      <td>0.472509</td>\n      <td>0.462295</td>\n      <td>0.588710</td>\n      <td>0.546713</td>\n      <td>0.592024</td>\n      <td>...</td>\n      <td>0.489431</td>\n      <td>0.445010</td>\n      <td>0.584445</td>\n      <td>0.652125</td>\n      <td>1.418717</td>\n      <td>0.507393</td>\n      <td>0.610276</td>\n      <td>0.666684</td>\n      <td>0.480519</td>\n      <td>0.530751</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/msmarco-bert-co-condensor</th>\n      <td>0.535823</td>\n      <td>0.540151</td>\n      <td>0.225705</td>\n      <td>0.651980</td>\n      <td>0.595589</td>\n      <td>0.487896</td>\n      <td>0.468041</td>\n      <td>0.535067</td>\n      <td>0.525086</td>\n      <td>0.590187</td>\n      <td>...</td>\n      <td>0.494336</td>\n      <td>0.460483</td>\n      <td>0.462056</td>\n      <td>0.444118</td>\n      <td>0.451855</td>\n      <td>1.412777</td>\n      <td>0.495614</td>\n      <td>0.459660</td>\n      <td>0.479899</td>\n      <td>0.512684</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/sentence-t5-large</th>\n      <td>0.521697</td>\n      <td>0.565155</td>\n      <td>0.197310</td>\n      <td>0.655143</td>\n      <td>0.649616</td>\n      <td>0.460349</td>\n      <td>0.447484</td>\n      <td>0.602207</td>\n      <td>0.542266</td>\n      <td>0.584934</td>\n      <td>...</td>\n      <td>0.467775</td>\n      <td>0.416189</td>\n      <td>0.511749</td>\n      <td>0.684795</td>\n      <td>0.523900</td>\n      <td>0.454242</td>\n      <td>1.397966</td>\n      <td>0.692518</td>\n      <td>0.491344</td>\n      <td>0.538427</td>\n    </tr>\n    <tr>\n      <th>sentence-transformers/sentence-t5-xl</th>\n      <td>0.526644</td>\n      <td>0.565993</td>\n      <td>0.194660</td>\n      <td>0.629109</td>\n      <td>0.680702</td>\n      <td>0.443204</td>\n      <td>0.439632</td>\n      <td>0.556886</td>\n      <td>0.538152</td>\n      <td>0.578010</td>\n      <td>...</td>\n      <td>0.456685</td>\n      <td>0.409267</td>\n      <td>0.484659</td>\n      <td>0.512316</td>\n      <td>0.626466</td>\n      <td>0.446312</td>\n      <td>0.724738</td>\n      <td>1.407957</td>\n      <td>0.477136</td>\n      <td>0.529059</td>\n    </tr>\n    <tr>\n      <th>thenlper/gte-base</th>\n      <td>1.118852</td>\n      <td>0.565083</td>\n      <td>0.206459</td>\n      <td>0.666170</td>\n      <td>0.644759</td>\n      <td>0.529239</td>\n      <td>0.541013</td>\n      <td>0.686315</td>\n      <td>1.198267</td>\n      <td>0.594267</td>\n      <td>...</td>\n      <td>0.495669</td>\n      <td>0.451392</td>\n      <td>0.465789</td>\n      <td>0.452752</td>\n      <td>0.458957</td>\n      <td>0.526197</td>\n      <td>0.562936</td>\n      <td>0.516471</td>\n      <td>1.423794</td>\n      <td>0.642169</td>\n    </tr>\n    <tr>\n      <th>thenlper/gte-large</th>\n      <td>0.618912</td>\n      <td>0.594474</td>\n      <td>0.203882</td>\n      <td>0.662110</td>\n      <td>0.657575</td>\n      <td>0.507029</td>\n      <td>0.532981</td>\n      <td>1.174485</td>\n      <td>0.674558</td>\n      <td>0.592834</td>\n      <td>...</td>\n      <td>0.491538</td>\n      <td>0.443071</td>\n      <td>0.482341</td>\n      <td>0.467944</td>\n      <td>0.460346</td>\n      <td>0.501386</td>\n      <td>0.569840</td>\n      <td>0.534219</td>\n      <td>0.609543</td>\n      <td>1.423009</td>\n    </tr>\n  </tbody>\n</table>\n<p>43 rows Ã— 43 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.570749798Z",
     "start_time": "2024-04-23T13:12:00.429225595Z"
    }
   },
   "id": "21d7b49d4fcf3727",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sanitize_metric_name(metric):\n",
    "    return metric.replace('/', '_').replace(' ', '_').replace('(', '').replace(')', '').replace('->', 'to').replace('(', '').replace(')', '')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.590505011Z",
     "start_time": "2024-04-23T13:12:00.449146298Z"
    }
   },
   "id": "2f86f33a56ca09c3",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:00.592786164Z",
     "start_time": "2024-04-23T13:12:00.451887600Z"
    }
   },
   "id": "f31c08b12366ba15",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fc95913e0d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fc95913e0d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fc95913e0d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fc95913e0d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fc95913e0d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x7fc9230d3100> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7fc91f431260> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fc95913e0d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "link = linkage(table, method=\"ward\")\n",
    "\n",
    "cluster = sns.clustermap(table, row_linkage=link, col_linkage=link, figsize=(20, 20), cmap='flare', annot=True, fmt=\".1f\", vmax=1)\n",
    "\n",
    "# make labels bigger and bold and rename in model X and model Y\n",
    "cluster.ax_heatmap.set_xticklabels(cluster.ax_heatmap.get_xticklabels(), fontsize=15, fontweight='bold')\n",
    "cluster.ax_heatmap.set_yticklabels(cluster.ax_heatmap.get_yticklabels(), fontsize=15, fontweight='bold')\n",
    "cluster.ax_heatmap.set_xlabel(\"Model Y\", fontsize=15, fontweight='bold')\n",
    "cluster.ax_heatmap.set_ylabel(\"Model X\", fontsize=15, fontweight='bold')\n",
    "\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/fig/nlp/{PREFIX}_mis_graph_clustermap_{sanitize_metric_name(METRIC)}.png\")\n",
    "# make sure the folder exists\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "cluster.savefig(path, bbox_inches='tight', dpi=300)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:03.009695326Z",
     "start_time": "2024-04-23T13:12:00.454235482Z"
    }
   },
   "id": "e9de21d7d8151fe0",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Community detection\n",
    "\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "G = nx.from_pandas_adjacency(table, create_using=nx.DiGraph)\n",
    "communities = list(greedy_modularity_communities(G))\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.949269315Z"
    }
   },
   "id": "af9abd03a454273e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.951192644Z"
    }
   },
   "id": "6b72ab68521c09da",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from netgraph import Graph, InteractiveGraph\n",
    "\n",
    "from networkx.algorithms.community import girvan_newman, modularity_max, louvain_communities\n",
    "\n",
    "G= nx.from_pandas_adjacency(table, create_using=nx.DiGraph)\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "communities = louvain_communities(G, resolution=1.1)\n",
    "communities = list(communities)\n",
    "# get a discrete color map\n",
    "\n",
    "cmap = sns.color_palette(\"flare\", as_cmap=True)\n",
    "\n",
    "avg_weight = {n : np.median([d[2]['weight'] for d in G.out_edges(n, data=True)]) for n in G.nodes()}\n",
    "avg_income = {n : np.median([d[2]['weight'] for d in G.in_edges(n, data=True)]) for n in G.nodes()}\n",
    "\n",
    "# normalize the average weight\n",
    "\n",
    "node_to_community = {node: i for i, community in enumerate(communities) for node in community}\n",
    "\n",
    "# node color using a color map\n",
    "# node_color = {node: cmap(i) for i, community in enumerate(communities) for node in community}\n",
    "\n",
    "# make average out going weight the node color\n",
    "node_color = {node: cmap(avg_weight[node]) for node in G.nodes()}\n",
    "node_edge_color = {node: cmap(avg_income[node]) for node in G.nodes()}\n",
    "\n",
    "\n",
    "node_labels = {node: node for node in G.nodes()}\n",
    "\n",
    "edge_color = {edge: G.edges[edge]['weight'] for edge in G.edges()}\n",
    "# normalize edge \n",
    "edge_color = {edge: cmap(edge_color[edge]) for edge in edge_color}\n",
    "max_edge = max(e[2]['weight'] for e in G.edges(data=True))\n",
    "\n",
    "\n",
    "# normalize edge alpha\n",
    "min_alpha = 0.01\n",
    "max_alpha = 0.4\n",
    "edge_alpha = {edge: G.edges[edge]['weight'] for edge in G.edges()}\n",
    "edge_alpha = {edge: (edge_alpha[edge] - min(edge_alpha.values())) / (max(edge_alpha.values()) - min(edge_alpha.values())) * (max_alpha - min_alpha) + min_alpha for edge in edge_alpha}\n",
    "\n",
    "# edge width\n",
    "min_edge_width = 0.01\n",
    "max_edge_width = 0.7\n",
    "edge_width = {edge: G.edges[edge]['weight'] for edge in G.edges()}\n",
    "edge_width = {edge: (edge_width[edge] - min(edge_width.values())) / (max(edge_width.values()) - min(edge_width.values())) * (max_edge_width - min_edge_width) + min_edge_width for edge in edge_width}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "\n",
    "graph = Graph(G, node_layout_kwargs=dict(node_to_community=node_to_community), node_layout=\"community\", node_color=node_color, node_labels=node_labels, edge_color=edge_color, ax=ax, node_label_fontdict={'fontsize': 15, 'fontweight': 'bold'}, node_edge_color=node_edge_color, edge_layout=\"curved\", edge_alpha=edge_alpha, arrows=True)\n",
    "\n",
    "\n",
    "\n",
    "# add white contour to all texts in the figure\n",
    "for text in plt.gca().texts:\n",
    "    text.set_path_effects([patheffects.Stroke(linewidth=4, foreground='white'), patheffects.Normal()])\n",
    "    \n",
    "# add color bar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=max_edge))\n",
    "sm._A = []\n",
    "# cbar = plt.colorbar(sm, ax=ax, orientation='horizontal', shrink=0.5, pad=0.05)\n",
    "\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/fig/nlp/{PREFIX}_mis_graph_community_{sanitize_metric_name(METRIC)}.png\")\n",
    "# make sure the folder exists\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "print(\"DOESNT SAVE ANYTHING SEE MODEL CLUSTERING!!!\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.953188147Z"
    }
   },
   "id": "aac793fb93d5417",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.955155104Z"
    }
   },
   "id": "641fb3bf60d63318",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictive_power = table.mean(axis=1).sort_values(ascending=False).to_frame()\n",
    "predictive_power['community'] = predictive_power.index.map(node_to_community)\n",
    "\n",
    "# keep top 1 for each community\n",
    "top_1 = predictive_power.groupby('community').head(1).sort_values(by=0, ascending=False).head(5)\n",
    "\n",
    "# get pair with the lowest predictive power\n",
    "\n",
    "top_table = table.loc[top_1.index, top_1.index]\n",
    "\n",
    "\n",
    "unrelated_pairs = []\n",
    "for k, m1 in enumerate(top_table.index):\n",
    "    for j, m2 in enumerate(top_table.columns):\n",
    "        if k < j:\n",
    "            v = max(top_table.loc[m1, m2], top_table.loc[m2, m1])\n",
    "            unrelated_pairs.append((m1, m2, v, predictive_power.loc[m1, 0], predictive_power.loc[m2, 0]))\n",
    "            \n",
    "unrelated_pairs = sorted(unrelated_pairs, key=lambda x: x[2], reverse=True)\n",
    "unrelated_pairs\n",
    "    \n",
    "    \n",
    "text = \"\"\n",
    "for pair in unrelated_pairs:\n",
    "    text += f\"'{pair[0]} {pair[1]}' \"\n",
    "    \n",
    "print(text)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.957089024Z"
    }
   },
   "id": "beed7c02c4eac2e4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.958954288Z"
    }
   },
   "id": "fb78a4848e291c64",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# classifcation_df = pd.read_csv('exported_data/classification_2.csv')\n",
    "classifcation_df = pd.read_csv('exported_data/classification_many.csv')\n",
    "\n",
    "display(classifcation_df)\n",
    "\n",
    "classifcation_df['model'] = classifcation_df['model'].apply(lambda x: eval(x)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.960880188Z"
    }
   },
   "id": "afa323d93350fdbb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "classification_ranking = classifcation_df.groupby('model')['success'].mean().sort_values(ascending=False).to_frame()\n",
    "\n",
    "classification_ranking['ranking'] = \"classification\"\n",
    "classification_ranking['rank'] = classification_ranking['success'].rank(ascending=False)\n",
    "# rename success to value\n",
    "classification_ranking = classification_ranking.rename(columns={'success': 'value'})\n",
    "classification_ranking = classification_ranking.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "informativeness_ranking = table.median(axis=1).sort_values(ascending=False).to_frame()\n",
    "\n",
    "# rename 0 to informativeness\n",
    "informativeness_ranking = informativeness_ranking.rename(columns={0: 'value'})\n",
    "\n",
    "informativeness_ranking['ranking'] = \"informativeness\"\n",
    "informativeness_ranking['rank'] = informativeness_ranking['value'].rank(ascending=False)\n",
    "informativeness_ranking = informativeness_ranking.reset_index()\n",
    "\n",
    "informativeness_ranking = informativeness_ranking.rename(columns={'index': 'model', 'model_1': 'model'})\n",
    "\n",
    "\n",
    "G= nx.from_pandas_adjacency(table, create_using=nx.DiGraph)\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "communities = louvain_communities(G, resolution=1.1)\n",
    "communities = list(communities)\n",
    "\n",
    "ranking = pd.concat([classification_ranking, informativeness_ranking], axis=0)\n",
    "\n",
    "# model to community\n",
    "node_to_community = {node: i for i, community in enumerate(communities) for node in community}\n",
    "\n",
    "ranking['community'] = ranking['model'].apply(lambda x: node_to_community[x] if x in node_to_community else -1)\n",
    "\n",
    "display(informativeness_ranking)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.962847545Z"
    }
   },
   "id": "aef8aed8299245a9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "toplot_info = ranking[ranking['ranking'] ==  \"informativeness\"]\n",
    "toplot_classif = ranking[ranking['ranking'] ==  \"classification\"]\n",
    "\n",
    "\n",
    "# rank both according to toplot_info value\n",
    "toplot_info = toplot_info.sort_values(by=\"value\", ascending=False)\n",
    "info_models = toplot_info['model'].values\n",
    "classifs_models = toplot_classif['model'].values\n",
    "\n",
    "\n",
    "# make intersection\n",
    "info_models = set(info_models)\n",
    "classifs_models = set(classifs_models)\n",
    "models = list(info_models.intersection(classifs_models))\n",
    "\n",
    "\n",
    "print(info_models)\n",
    "print(classifs_models)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.964809528Z"
    }
   },
   "id": "4945ee712ae29630",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# sort according to informativeness\n",
    "models = sorted(models, key=lambda x: toplot_info.loc[toplot_info['model'] == x, 'value'].values[0], reverse=True)\n",
    "print(models)\n",
    "toplot_classif = toplot_classif.set_index('model').loc[models].reset_index()\n",
    "toplot_info = toplot_info[toplot_info['model'].isin(models)]\n",
    "\n",
    "spectral_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "model_colors = {m : k for k, m in enumerate(models)}\n",
    "# normalize colors\n",
    "model_colors = {m: spectral_cmap(k / len(model_colors)) for m, k in model_colors.items()}\n",
    "\n",
    "toplot_info['color'] = toplot_info['model'].map(model_colors)\n",
    "toplot_classif['color'] = toplot_classif['model'].map(model_colors)\n",
    "\n",
    "\n",
    "# make ranked bar plot\n",
    "fig, ax = plt.subplot_mosaic([['ranking_classif', 'ranking_informativeness']], sharey=True, sharex=False, figsize=(15, 7))\n",
    "\n",
    "# horizontal bar plot, with shared y axis\n",
    "# share y axis\n",
    "\n",
    "hatches = ['/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*']*2\n",
    "\n",
    "sns.barplot(data=toplot_info, y=\"model\", x=\"value\", ax=ax['ranking_informativeness'], orient='h', palette=model_colors, hue=\"model\")\n",
    "for i, bar in enumerate(ax['ranking_informativeness'].patches):\n",
    "    # hatche based on community\n",
    "    bar.set_hatch(hatches[toplot_info.iloc[i]['community']])\n",
    "\n",
    "# set xlim\n",
    "ax['ranking_informativeness'].set_xlim(0.3, 0.65)\n",
    "\n",
    "sns.barplot(data=toplot_classif, y=\"model\", x=\"value\", ax=ax['ranking_classif'], orient='h', hue=\"model\", palette=model_colors)\n",
    "\n",
    "for i, bar in enumerate(ax['ranking_classif'].patches):\n",
    "    # hatche based on community\n",
    "    bar.set_hatch(hatches[toplot_classif.iloc[i]['community']])\n",
    "\n",
    "# set xlim\n",
    "ax['ranking_classif'].set_xlim(0.5, 0.7)\n",
    "\n",
    "# share y axis:\n",
    "ax['ranking_informativeness'].sharey(ax['ranking_classif'])\n",
    "ax['ranking_informativeness'].tick_params(labelleft=False)\n",
    "ax['ranking_informativeness'].set_ylabel(\"\")\n",
    "\n",
    "# make yticks bigger\n",
    "ax['ranking_classif'].tick_params(axis='y', labelsize=15)\n",
    "\n",
    "# remove xticks\n",
    "ax['ranking_classif'].tick_params(labelbottom=False)\n",
    "ax['ranking_classif'].set_xlabel(\"\")\n",
    "ax['ranking_informativeness'].tick_params(labelbottom=False)\n",
    "ax['ranking_informativeness'].set_xlabel(\"\")\n",
    "\n",
    "# remvoe ylabels\n",
    "ax['ranking_classif'].set_ylabel(\"\")\n",
    "\n",
    "# set titles\n",
    "ax['ranking_classif'].set_title(\"Classification ranking\", fontsize=15, fontweight='bold')\n",
    "ax['ranking_informativeness'].set_title(\"Informativeness ranking\", fontsize=15, fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/fig/nlp/nlp_models_rankings_{sanitize_metric_name(METRIC)}.png\")\n",
    "# make sure the folder exists\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "######## \n",
    "\n",
    "\n",
    "# deduplicate the ranking\n",
    "for_pivot = ranking.groupby(['model', 'community', 'ranking']).first().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "ranking_pivot = for_pivot.pivot(columns=\"ranking\", values=\"rank\", index=['model', 'community']).reset_index()\n",
    "# select models\n",
    "\n",
    "display(ranking_pivot)\n",
    "\n",
    "print(models)\n",
    "ranking_pivot = ranking_pivot[ranking_pivot['model'].isin(models)]\n",
    "\n",
    "\n",
    "# remove community -1\n",
    "ranking_pivot = ranking_pivot[ranking_pivot['community'] != -1]\n",
    "ranking_pivot['community'] = ranking_pivot['community'].astype(str)\n",
    "\n",
    "# sort\n",
    "ranking_pivot = ranking_pivot.sort_values(by=\"community\")\n",
    "\n",
    "# rename model and community\n",
    "ranking_pivot = ranking_pivot.rename(columns={'model': 'Model', 'community': 'Community', 'classification': 'Classification', 'informativeness': 'Informativeness'})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "\n",
    "\n",
    "g = sns.scatterplot(data=ranking_pivot, x=\"Classification\", y=\"Informativeness\", style=\"Community\", hue=\"Model\", s=600, ax=ax, legend=False, palette=model_colors)\n",
    "# plot linear regression\n",
    "sns.regplot(data=ranking_pivot, x=\"Classification\", y=\"Informativeness\", scatter=False, ax=ax)\n",
    "# annotate with correlation\n",
    "correlation = ranking_pivot['Classification'].corr(ranking_pivot['Informativeness'])\n",
    "ax.annotate(f\"Spearman correlation: {correlation:.2f}\", xy=(0.5, 0.1), xycoords='axes fraction', fontsize=15, fontweight='bold')\n",
    "\n",
    "# make labels bigger and bold and rename in model X and model Y\n",
    "ax.set_xlabel(\"Classification ranking\", fontsize=15, fontweight='bold')\n",
    "ax.set_ylabel(\"Informativeness ranking\", fontsize=15, fontweight='bold')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/fig/nlp/{PREFIX}_nlp_models_ranking_correlation_{sanitize_metric_name(METRIC)}.png\")\n",
    "# make sure the folder exists\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.966960222Z"
    }
   },
   "id": "56c2e4c5e655422b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pivot the table\n",
    "ranking_pivot = ranking.pivot(columns=\"ranking\", values=\"rank\", index=['model', 'community']).reset_index()\n",
    "# remove community -1\n",
    "ranking_pivot = ranking_pivot[ranking_pivot['community'] != -1]\n",
    "\n",
    "ranking_pivot['community'] = ranking_pivot['community'].astype(str)\n",
    "\n",
    "\n",
    "# sort\n",
    "ranking_pivot = ranking_pivot.sort_values(by=\"community\")\n",
    "\n",
    "# rename model and community\n",
    "ranking_pivot = ranking_pivot.rename(columns={'model': 'Model', 'community': 'Community', 'classification': 'Classification', 'informativeness': 'Informativeness'})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "g = sns.scatterplot(data=ranking_pivot, x=\"Classification\", y=\"Informativeness\", style=\"Community\", hue=\"Model\", s=600, ax=ax)\n",
    "\n",
    "# plot linear regression\n",
    "sns.regplot(data=ranking_pivot, x=\"Classification\", y=\"Informativeness\", scatter=False, ax=ax)\n",
    "# annotate with correlation\n",
    "correlation = ranking_pivot['Classification'].corr(ranking_pivot['Informativeness'])\n",
    "ax.annotate(f\"Spearman correlation: {correlation:.2f}\", xy=(0.5, 0.1), xycoords='axes fraction', fontsize=15, fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "# legend outside with 2 columns, below the plot\n",
    "plt.legend(bbox_to_anchor=(.0, -0.15), loc=2, borderaxespad=0., ncol=3)\n",
    "\n",
    "# rename x and y labels and make bold and bigger\n",
    "\n",
    "ax.set_xlabel(\"Classification ranking\", fontsize=15, fontweight='bold')\n",
    "ax.set_ylabel(\"Informativeness ranking\", fontsize=15, fontweight='bold')\n",
    "\n",
    "\n",
    "# save the figure\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/fig/nlp/{PREFIX}_nlp_models_ranking_correlation_{sanitize_metric_name(METRIC)}_legend.png\")\n",
    "# make sure the folder exists\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.969232653Z"
    }
   },
   "id": "cce5dae70a8c818c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# pivot the table\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "ranking_pivot = ranking.pivot(columns=\"ranking\", values=\"value\", index=['model', 'community']).reset_index()\n",
    "\n",
    "sns.scatterplot(data=ranking_pivot, x=\"classification\", y=\"informativeness\", style=\"community\", hue=\"model\", s=200, ax=ax)\n",
    "\n",
    "# plot linear regression\n",
    "sns.regplot(data=ranking_pivot, x=\"classification\", y=\"informativeness\", scatter=False, ax=ax)\n",
    "# annotate with correlation\n",
    "correlation = ranking_pivot['classification'].corr(ranking_pivot['informativeness'])\n",
    "ax.annotate(f\"Correlation: {correlation:.2f}\", xy=(0.5, 0.1), xycoords='axes fraction', fontsize=15, fontweight='bold')\n",
    "\n",
    "# legend outside\n",
    "\n",
    "plt.legend(bbox_to_anchor=(.0, -0.15), loc=2, borderaxespad=0., ncol=3)\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/fig/nlp/{PREFIX}_nlp_models_acc_info_correlation_{sanitize_metric_name(METRIC)}.png\")\n",
    "# make sure the folder exists\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.971341833Z"
    }
   },
   "id": "9611f612726ccae0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "toplot = ranking[ranking['ranking'] ==  \"classification\"]\n",
    "toplot['community'] = toplot['community'].astype(str)\n",
    "# remove community -1\n",
    "toplot = toplot[toplot['community'] != \"-1\"]\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "toplot = toplot.sort_values(by=\"community\")\n",
    "sns.barplot(data=toplot, x=\"model\", y=\"value\", ax=ax, hue=\"community\")\n",
    "# rotate x labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0.5, 0.7)\n",
    "\n",
    "# rename x and y labels and make bold and bigger\n",
    "plt.xlabel(\"Model\", fontsize=15, fontweight='bold')\n",
    "plt.ylabel(\"Avg. Accuracy\", fontsize=15, fontweight='bold')\n",
    "\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/fig/nlp/{PREFIX}_classifs_perfs_per_community_{sanitize_metric_name(METRIC)}.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.973389049Z"
    }
   },
   "id": "64a9397f9c69d848",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Classification clustering"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.975426371Z"
    }
   },
   "id": "759ae43d9512adcd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.977429337Z"
    }
   },
   "id": "2427f9f78f10ef52",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_classification = pd.read_csv('exported_data/classification_many.csv')\n",
    "full_classification = full_classification[~full_classification['dataset'].str.contains('clinc') ]\n",
    "\n",
    "\n",
    "full_classification['model'] = full_classification['model'].apply(lambda x: eval(x)[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.979467791Z"
    }
   },
   "id": "a3438be6a566613d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.981545130Z"
    }
   },
   "id": "876cde9687d40509",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# add community to the classification\n",
    "full_classification['community'] = full_classification['model'].apply(lambda x: str(node_to_community[x]) if x in node_to_community else -1)\n",
    "\n",
    "# remove if community is -1\n",
    "full_classification = full_classification[full_classification['community'] != -1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.983655069Z"
    }
   },
   "id": "587385d6b1fb7dd1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(node_to_community)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.985828804Z"
    }
   },
   "id": "28e30e24dd3c0040",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_classification"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.987909276Z"
    }
   },
   "id": "2a974df0fe62ab1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "classification_perfs = full_classification.join(ranking.set_index('model'), on='model', how='left')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:02.989947819Z"
    }
   },
   "id": "cd77d1777f6a5275",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.009373042Z"
    }
   },
   "id": "39e6304526e2b69d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.009656604Z"
    }
   },
   "id": "442dcf6111acd559",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load mteb data\n",
    "\n",
    "mteb = pd.read_csv('exported_data/df_mteb_avg.csv')\n",
    "mteb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:12:03.115987692Z",
     "start_time": "2024-04-23T13:12:03.009851607Z"
    }
   },
   "id": "330281cdb1a1b277",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mteb\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_url_from_html_link(html):\n",
    "    return re.findall(r'href=[\\'\"]?([^\\'\" >]+)', html)[0]\n",
    "\n",
    "\n",
    "mteb['model'] = mteb['Model'].apply(extract_url_from_html_link).apply(lambda x: \"/\".join(x.split('/')[-2:]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.010497542Z"
    }
   },
   "id": "f95f4c25cfd0ee86",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "avg_results = table.mean(axis=1).sort_values(ascending=False).to_frame().reset_index().rename(columns={0: METRIC, 'model_1': 'model'}).merge(mteb, on='model', how='left').sort_values(METRIC, ascending=False)\n",
    "\n",
    "# remove unnamed columns\n",
    "avg_results = avg_results.loc[:, ~avg_results.columns.str.contains('^Unnamed')]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.010623792Z"
    }
   },
   "id": "749daa24ab0cd89",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "print(list(avg_results.columns))\n",
    "\n",
    "METRICS = ['Average (56 datasets)', 'Classification Average (12 datasets)', 'Clustering Average (11 datasets)', 'Reranking Average (4 datasets)', 'Retrieval Average (15 datasets)', 'STS Average (10 datasets)'] + [METRIC]\n",
    "\n",
    "METADATA = ['model', 'Rank', 'Model', 'Model Size (GB)', 'Embedding Dimensions', 'Max Tokens']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.010734572Z"
    }
   },
   "id": "1277d4061fae8185",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(avg_results[METRICS].corr(), annot=True, cmap='coolwarm')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.010877811Z"
    }
   },
   "id": "63df6c94835d74d9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# melt\n",
    "avg_results_pivot = avg_results.melt(id_vars=METADATA+[METRIC], value_vars=set(METRICS)-set(METRIC), var_name=\"Metric\", value_name=\"Accuracy\")\n",
    "# remove metric about summarization\n",
    "\n",
    "\n",
    "avg_results_pivot[\"Community\"] = avg_results_pivot['model'].map(node_to_community)\n",
    "\n",
    "# sns.relplot(data=avg_results_pivot, x=METRIC, y=\"Value\", col=\"Metric\", kind=\"scatter\", col_wrap=3, height=10, aspect=1.5, hue=\"model\", facet_kws={'sharey': False, 'sharex': False}, s=500)\n",
    "\n",
    "g = sns.FacetGrid(data=avg_results_pivot, col=\"Metric\", col_wrap=3, height=5, aspect=1.5, sharey=False, sharex=False)\n",
    "\n",
    "def scatterplot_relplot(x, y, **kwargs):\n",
    "    data = kwargs.pop('data')\n",
    "    sns.scatterplot(x=x, y=y, data=data, ax=plt.gca(), **kwargs)\n",
    "    # regplot\n",
    "    sns.regplot(x=x, y=y, data=data, scatter=False, ax=plt.gca())\n",
    "    \n",
    "    # annotate with correlation\n",
    "    correlation = data[x].corr(data[y])\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(f\"Correlation: {correlation:.2f}\", xy=(0.5, 0.1), xycoords='axes fraction', fontsize=15, fontweight='bold')\n",
    "    \n",
    "g.map_dataframe(scatterplot_relplot, x=METRIC, y=\"Accuracy\", hue=\"model\", s=500, style=\"Community\")\n",
    "\n",
    "# change title template \n",
    "g.set_titles(template=\"{col_name}\", size=15, fontweight='bold')\n",
    "\n",
    "# make x and y labels bigger and bold\n",
    "g.set_xlabels(METRIC, fontsize=15, fontweight='bold')\n",
    "g.set_ylabels(\"Accuracy\", fontsize=15, fontweight='bold')\n",
    "\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/fig/nlp/{PREFIX}_mteb_correlation_{sanitize_metric_name(METRIC)}.png\")\n",
    "# make sure the folder exists\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "g.savefig(path, bbox_inches='tight', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.010994483Z"
    }
   },
   "id": "d95dce9f483fc954",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Correlation between metric and embedding dimensions\n",
    "sns.scatterplot(data=avg_results, x=METRIC, y=\"Embedding Dimensions\", hue=\"model\", s=500, legend=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.011104108Z"
    }
   },
   "id": "409676708d029c9d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.011218742Z"
    }
   },
   "id": "1bd8bcf88071adb9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Make metadata table\n",
    "\n",
    "\n",
    "metadata_table = avg_results.copy()[METADATA+METRICS]\n",
    "\n",
    "metadata_table = metadata_table.drop([\"Model\", \"Rank\"], axis=1).rename(columns={'model': 'Model'})\n",
    "metadata_table = metadata_table.set_index('Model')\n",
    "metadata_table['Embedding Dimensions'] = df.groupby('model_2')['d_2'].first()\n",
    "\n",
    "# rename I(X_1->X_2)/d_2 to Informativeness\n",
    "\n",
    "metadata_table = metadata_table.rename(columns={METRIC: \"Info.\"})\n",
    "metadata_table = metadata_table.rename(columns={'Average (56 datasets)': 'Average', 'Classification Average (12 datasets)': 'Classification', 'Clustering Average (11 datasets)': 'Clustering', 'Reranking Average (4 datasets)': 'Reranking', 'Retrieval Average (15 datasets)': 'Retrieval', 'STS Average (10 datasets)': 'STS', \"Embedding Dimensions\": \"Dim.\", \"Model Size (GB)\" : \"Size (GB)\", \"Max Tokens\": \"Max Tokens\"})\n",
    "\n",
    "# make \\\\url with hugginface url and model name\n",
    "metadata_table = metadata_table.reset_index()\n",
    "metadata_table['Model'] = metadata_table['Model'].apply(lambda x: x.replace('_', r'\\_')).apply(lambda x: \"\\\\href{https://huggingface.co/\" + x + \"}\" + \"{\" + x + \"}\")\n",
    "metadata_table = metadata_table.set_index('Model')\n",
    "\n",
    "style = metadata_table.style.format({'Model Size (GB)': \"{:.2f}\", 'Embedding Dimensions': \"{:.0f}\",  \"Max Tokens\": \"{:.0f}\", \"Info.\" : \"{:.2f}\", 'Average': \"{:.2f}\", 'Classification': \"{:.2f}\", 'Clustering': \"{:.2f}\", 'Reranking': \"{:.2f}\", 'Retrieval': \"{:.2f}\", 'STS': \"{:.2f}\", 'Dim.': \"{:.0f}\", 'Size (GB)': \"{:.2f}\", 'Max Tokens': \"{:.0f}\"}, na_rep=\"N/A\")\n",
    "\n",
    "# make Info. bold\n",
    "style = style.set_properties(subset=[\"Info.\"], **{\"bfseries\" :\"\"})\n",
    "\n",
    "# escape\n",
    "style = style.format_index()\n",
    "\n",
    "path = Path(f\"../../../papers/emir-embedding-comparison/tables/nlp/{PREFIX}_mteb_metadata_table_{sanitize_metric_name(METRIC)}.tex\")\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "latex = style.to_latex(clines=\"skip-last;data\", sparse_index=True, caption=\"Summary of the evaluated embedders with their performance on the MTEB benchmark along with their informativeness.\", label=\"tab:nlp_metadata_table\", hrules=True)\n",
    "\n",
    "# add resizebox to the latex\n",
    "latex = latex.replace(\"\\\\begin{tabular}\", \"\\\\resizebox{\\\\textwidth}{!}{ \\\\begin{tabular}\")\n",
    "latex = latex.replace(\"\\\\end{tabular}\", \"\\\\end{tabular}\\n}\")\n",
    "\n",
    "# save the latex\n",
    "path.write_text(latex)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.011322929Z"
    }
   },
   "id": "a1aa58a77c21ee3f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# ['llmrails/ember-v1', 'jamesgpt1/sf_model_e5', 'infgrad/stella-base-en-v2', 'WhereIsAI/UAE-Large-V1', 'avsolatorio/GIST-Embedding-v0', 'BAAI/bge-base-en-v1.5', 'thenlper/gte-base', 'thenlper/gte-large', 'intfloat/e5-large-v2', 'sentence-transformers/gtr-t5-large', 'sentence-transformers/gtr-t5-xl', 'sentence-transformers/sentence-t5-large', 'sentence-transformers/gtr-t5-base', 'sentence-transformers/sentence-t5-xl', 'TaylorAI/gte-tiny', 'SmartComponents/bge-micro-v2', 'intfloat/e5-small', 'sentence-transformers/msmarco-bert-co-condensor', 'intfloat/multilingual-e5-small', 'sentence-transformers/all-distilroberta-v1', 'princeton-nlp/sup-simcse-bert-base-uncased', 'sentence-transformers/all-MiniLM-L6-v2', 'sentence-transformers/all-mpnet-base-v2', 'izhx/udever-bloom-560m', 'sentence-transformers/LaBSE', 'sentence-transformers/average_word_embeddings_komninos', 'sentence-transformers/average_word_embeddings_glove.6B.300d', 'sentence-transformers/allenai-specter']\n",
    "\n",
    "# dict model name to citation name\n",
    "\n",
    "model_citation = {\n",
    "    'llmrails/ember-v1': 'EMBER',\n",
    "    'jamesgpt1/sf_model_e5': 'SF-Model',\n",
    "    'infgrad/stella-base-en-v2': 'STELLA',\n",
    "    'WhereIsAI/UAE-Large-V1': 'UAE-Large',\n",
    "    'avsolatorio/GIST-Embedding-v0': 'GIST',\n",
    "    'BAAI/bge-base-en-v1.5': 'BGE-Base',\n",
    "    'thenlper/gte-base': 'GTE-Base',\n",
    "    'thenlper/gte-large': 'GTE-Large',\n",
    "    'intfloat/e5-large-v2': 'E5-Large',\n",
    "    'sentence-transformers/gtr-t5-large': 'GTR-T5-Large',\n",
    "    'sentence-transformers/gtr-t5-xl': 'GTR-T5-XL',\n",
    "    'sentence-transformers/sentence-t5-large': 'Sentence-T5-Large',\n",
    "    'sentence-transformers/gtr-t5-base': 'GTR-T5-Base',\n",
    "    'sentence-transformers/sentence-t5-xl': 'Sentence-T5-XL',\n",
    "    'TaylorAI/gte-tiny': 'GTE-Tiny',\n",
    "    'SmartComponents/bge-micro-v2': 'BGE-Micro',\n",
    "    'intfloat/e5-small': 'E5-Small',\n",
    "    'sentence-transformers/msmarco-bert-co-condensor': 'MSMARCO-BERT',\n",
    "    'intfloat/multilingual-e5-small': 'Multilingual-E5-Small',\n",
    "    'sentence-transformers/all-distilroberta-v1': 'DistilRoBERTa',\n",
    "    'princeton-nlp/sup-simcse-bert-base-uncased': 'SimCSE',\n",
    "    'sentence-transformers/all-MiniLM-L6-v2': 'MiniLM-L6',\n",
    "    'sentence-transformers/all-mpnet-base-v2': 'MPNet-Base',\n",
    "    'izhx/udever-bloom-560m': 'UDEVER-Bloom',\n",
    "    'sentence-transformers/LaBSE': 'LaBSE',\n",
    "    'sentence-transformers/average_word_embeddings_komninos': 'Komninos',\n",
    "    'sentence-transformers/average_word_embeddings_glove.6B.300d': 'GloVe',\n",
    "    'sentence-transformers/all\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.011427130Z"
    }
   },
   "id": "69f484fe274c0fea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.011535102Z"
    }
   },
   "id": "b60cd7e9e812a894",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.011640150Z"
    }
   },
   "id": "cbe79b53982df5a0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.011872815Z"
    }
   },
   "id": "d15c16c132bf2d39",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.011991161Z"
    }
   },
   "id": "853fbaa2dfbab30d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.013362182Z"
    }
   },
   "id": "1dec1a7d4241d76e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.058878389Z"
    }
   },
   "id": "81219da34d80ebaa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.059152312Z"
    }
   },
   "id": "9e72ba5522cfa9dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-23T13:12:03.059335326Z"
    }
   },
   "id": "1e570ef2d38a3ead"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
