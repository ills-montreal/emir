constants:
  seed: &seed 42
  raise_train_error: true   # Whether the code should raise an error if it crashes during training

datamodule:
  df_path: data/Molecules/QM9/qm9_certain_structures.csv
  cache_data_path: data/Molecules/QM9/goli_cache.cache
  label_cols: ['mu']
  smiles_col: smiles

  # Featurization
  featurization_n_jobs: 2
  featurization_progress: True
  featurization:
    atom_property_list_onehot: [atomic-number]
    atom_property_list_float: []
    edge_property_list: []
    add_self_loop: False
    explicit_H: True
    use_bonds_weights: False

  # Train, val, test parameters
  split_val: 0.13565592252
  split_test: 0.09999923565
  split_seed: *seed
  splits_path: null
  batch_size_train_val: 128
  batch_size_test: 256

  # Data loading
  num_workers: 0
  pin_memory: False
  persistent_workers: False  # Keep True on Windows if running multiple workers


architecture:
  model_type: fulldglnetwork
  pre_nn:   # Set as null to avoid a pre-nn network
    out_dim: &middle_dim 90
    hidden_dims: *middle_dim
    depth: 0
    activation: relu
    last_activation: none
    dropout: &dropout 0.
    batch_norm: &batch_norm True
    last_batch_norm: *batch_norm
    residual_type: none

  gnn:  # Set as null to avoid a post-nn network
    out_dim: *middle_dim
    hidden_dims: *middle_dim
    depth: 4
    activation: relu
    last_activation: none
    dropout: *dropout
    batch_norm: *batch_norm
    last_batch_norm: *batch_norm
    residual_type: simple
    pooling: 'sum'
    virtual_node: none
    layer_type: 'pna-msgpass'
    layer_kwargs:
      # num_heads: 3
      aggregators: [mean, max, min, std]
      scalers: [identity, amplification, attenuation]

  post_nn:
    out_dim: 1
    hidden_dims: *middle_dim
    depth: 0
    activation: relu
    last_activation: none
    dropout: *dropout
    batch_norm: *batch_norm
    last_batch_norm: False
    residual_type: none


predictor:
  metrics_on_progress_bar: ["mae", "pearsonr", "f1 < 0", "precision < 0"]
  loss_fun: mse
  random_seed: *seed
  optim_kwargs:
    lr: 1.0e-5
    weight_decay: 3.0e-16
  lr_reduce_on_plateau_kwargs:
    factor: 0.5
    patience: 25
    min_lr: 1.e-5
  scheduler_kwargs:
    monitor: &monitor loss/val
    frequency: 1
  target_nan_mask: 0 # null: no mask, 0: 0 mask, ignore: ignore nan values from loss


metrics:
  - name: mae
    metric: mae
    threshold_kwargs: null

  - name: pearsonr
    metric: pearsonr
    threshold_kwargs: null

  - name: f1 < 0
    metric: f1
    num_classes: 2
    average: micro
    threshold_kwargs: &threshold_0
      operator: lower
      threshold: 0
      th_on_preds: True
      th_on_target: True

  - name: f1 < -1
    metric: f1
    num_classes: 2
    average: micro
    threshold_kwargs:
      operator: lower
      threshold: -1
      th_on_preds: True
      th_on_target: True

  - name: precision < 0
    metric: precision
    class_reduction: micro
    threshold_kwargs: *threshold_0

trainer:
  logger:
    save_dir: logs/QM9_bench_gnn
  early_stopping:
    monitor: *monitor
    min_delta: 0
    patience: 200
    mode: &mode min
  model_checkpoint:
    dirpath: models_checkpoints/micro_QM9/
    filename: "bob"
    monitor: *monitor
    mode: *mode
    save_top_k: 1
    period: 1
  trainer:
    max_epochs: 2000
    min_epochs: 100
    gpus: 1

    